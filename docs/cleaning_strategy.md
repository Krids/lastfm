# Data Cleaning Strategy for Last.fm Session Analysis

## ðŸŽ¯ **Pipeline Overview**

Based on your analysis results and **ADR-005 Hybrid Data Cleaning Approach**, here's a comprehensive data cleaning plan focused on session analysis requirements.

---

## ðŸ”§ **Data Cleaning Pipeline Steps**

### **Stage 1: Critical Field Validation (FAIL-FAST)**
**Objective**: Reject records that make session analysis impossible

#### **1.1 User ID Validation**
- âœ… **Must Have**: Non-null, non-empty, non-blank validation
- âœ… **Must Have**: Length validation (reasonable bounds: 3-50 characters)
- âœ… **Must Have**: Character set validation (alphanumeric + allowed special chars)
- âœ… **Must Have**: Consistent format validation (all follow same pattern)

#### **1.2 Timestamp Validation**  
- âœ… **Must Have**: Parse ISO 8601 format with timezone handling
- âœ… **Must Have**: Validate timestamp is within reasonable bounds (e.g., 2000-2030)
- âœ… **Must Have**: Detect and reject unparseable timestamps
- âœ… **Must Have**: Convert all to UTC for consistent session calculation
- âœ… **Must Have**: Validate chronological ordering per user (detect time travel)

### **Stage 2: High-Priority Cleansing (CLEANSE & CONTINUE)**
**Objective**: Standardize track identification for accurate song counting

#### **2.1 Artist Name Standardization**
- âœ… **Must Have**: Trim whitespace and normalize spacing
- âœ… **Must Have**: Remove control characters (tabs, newlines, carriage returns)
- âœ… **Must Have**: Handle null/empty values â†’ default to "Unknown Artist"
- âœ… **Must Have**: Length limits (reasonable maximum: 500 chars)
- âœ… **Must Have**: Unicode normalization for international characters
- âœ… **Must Have**: Case-insensitive deduplication hints

#### **2.2 Track Name Standardization**
- âœ… **Must Have**: Identical processing as artist names
- âœ… **Must Have**: Handle null/empty values â†’ default to "Unknown Track"
- âœ… **Must Have**: Remove common noise patterns (feat., featuring, remix indicators)
- âœ… **Must Have**: Standardize punctuation variations

#### **2.3 Track Identity Resolution**
- âœ… **Must Have**: Create composite track key hierarchy:
  1. Use Track MBID if available (88.7% coverage)
  2. Use Artist MBID + track name if track MBID missing (8.2% coverage)  
  3. Use standardized "artist_name â€” track_name" for remainder (3.1% coverage)
- âœ… **Must Have**: Validate MBID format if present (UUID pattern)
- âœ… **Must Have**: Handle case where both MBID and names are problematic

### **Stage 3: Quality Enhancement (ACCEPT & IMPROVE)**
**Objective**: Improve data quality without blocking processing

#### **3.1 Duplicate Detection**
- âœ… **Must Have**: Identify exact duplicates (same user, timestamp, track)
- âœ… **Must Have**: Identify near-duplicates (same user, track, within 5 seconds)
- âœ… **Must Have**: Configurable deduplication strategy (keep first, keep last, merge)

#### **3.2 Data Enrichment**
- âœ… **Must Have**: Generate session-relevant metadata (play sequence numbers)
- âœ… **Must Have**: Calculate time gaps between consecutive plays per user
- âœ… **Must Have**: Flag potential data quality issues for monitoring

---

## ðŸ§ª **Comprehensive Test Strategy**

### **Unit Tests: Critical Field Validation**

#### **User ID Validation Tests**
```scala
class UserIdValidationTests {
  // Happy Path
  âœ… "should accept valid alphanumeric user IDs"
  âœ… "should accept user IDs with allowed special characters"
  
  // Boundary Cases
  âœ… "should reject null user ID"
  âœ… "should reject empty string user ID" 
  âœ… "should reject whitespace-only user ID"
  âœ… "should reject user ID exceeding maximum length"
  âœ… "should reject user ID below minimum length"
  
  // Edge Cases
  âœ… "should handle user IDs with leading/trailing whitespace"
  âœ… "should reject user IDs with control characters"
  âœ… "should reject user IDs with SQL injection patterns"
  âœ… "should handle Unicode characters in user IDs"
  âœ… "should validate consistent user ID format across dataset"
}
```

#### **Timestamp Validation Tests**
```scala
class TimestampValidationTests {
  // Happy Path
  âœ… "should parse ISO 8601 timestamps with Z suffix"
  âœ… "should parse ISO 8601 timestamps with timezone offsets"
  âœ… "should convert all timestamps to UTC consistently"
  
  // Format Variations
  âœ… "should handle timestamps with milliseconds"
  âœ… "should handle timestamps without milliseconds" 
  âœ… "should reject non-ISO format timestamps"
  âœ… "should handle different timezone formats (+00:00, Z, GMT)"
  
  // Boundary Cases
  âœ… "should reject null timestamps"
  âœ… "should reject empty string timestamps"
  âœ… "should reject unparseable timestamp strings"
  âœ… "should reject timestamps before reasonable start date (2000)"
  âœ… "should reject timestamps after reasonable end date (2030)"
  
  // Edge Cases - Time Logic
  âœ… "should handle leap seconds correctly"
  âœ… "should handle daylight saving time transitions"
  âœ… "should handle midnight boundary crossings"
  âœ… "should detect chronological violations per user"
  âœ… "should handle identical timestamps for same user"
  âœ… "should handle timezone inconsistencies in same user data"
}
```

### **Unit Tests: Data Standardization**

#### **Artist/Track Name Cleaning Tests**
```scala
class NameStandardizationTests {
  // Basic Cleaning
  âœ… "should trim leading and trailing whitespace"
  âœ… "should normalize multiple spaces to single space"
  âœ… "should remove tab characters"
  âœ… "should remove newline and carriage return characters"
  âœ… "should handle null values with default replacement"
  âœ… "should handle empty strings with default replacement"
  
  // Unicode and International Characters
  âœ… "should preserve valid Unicode characters (Greek, Cyrillic)"
  âœ… "should normalize Unicode combining characters"
  âœ… "should handle accented characters consistently"
  âœ… "should preserve emoji and special music symbols"
  
  // Length and Safety
  âœ… "should truncate extremely long names to reasonable limit"
  âœ… "should handle names with only special characters"
  âœ… "should remove dangerous control characters"
  âœ… "should handle mixed character encodings"
  
  // Music-Specific Patterns
  âœ… "should standardize 'feat.' vs 'featuring' vs 'ft.'"
  âœ… "should handle remix indicators consistently"
  âœ… "should preserve important punctuation (apostrophes, hyphens)"
  âœ… "should handle case variations consistently"
}
```

#### **Track Identity Resolution Tests**
```scala
class TrackIdentityTests {
  // MBID Priority Logic
  âœ… "should use Track MBID when available"
  âœ… "should fall back to Artist MBID + track name when track MBID missing"
  âœ… "should fall back to name combination when no MBIDs available"
  âœ… "should validate MBID format when present"
  
  // MBID Validation
  âœ… "should accept valid UUID format MBIDs"
  âœ… "should reject malformed MBID strings"
  âœ… "should handle null MBIDs gracefully"
  âœ… "should handle empty string MBIDs"
  
  // Composite Key Generation
  âœ… "should generate consistent track keys for identical data"
  âœ… "should generate different keys for different tracks"
  âœ… "should handle special characters in composite keys"
  âœ… "should create deterministic keys for same input"
  
  // Edge Cases
  âœ… "should handle tracks with MBID but no names"
  âœ… "should handle tracks with names but invalid MBIDs"
  âœ… "should handle completely missing track information"
  âœ… "should preserve enough information for later analysis"
}
```

### **Integration Tests: Pipeline Orchestration**

#### **End-to-End Cleaning Tests**
```scala
class CleaningPipelineTests {
  // Pipeline Flow
  âœ… "should process valid records through all stages"
  âœ… "should reject records at appropriate stage"
  âœ… "should maintain record processing order"
  âœ… "should handle mixed valid/invalid batches"
  
  // Error Handling
  âœ… "should collect and report all validation errors"
  âœ… "should continue processing after recoverable errors"
  âœ… "should stop processing on critical configuration errors"
  âœ… "should provide detailed error context for debugging"
  
  // Performance
  âœ… "should process large datasets within memory limits"
  âœ… "should maintain consistent performance across data sizes"
  âœ… "should handle skewed data distributions efficiently"
  
  // Quality Reporting
  âœ… "should generate comprehensive quality reports"
  âœ… "should track cleaning statistics accurately"
  âœ… "should identify data quality trends"
}
```

### **Property-Based Tests (ScalaCheck)**

#### **Data Integrity Properties**
```scala
class DataCleaningProperties {
  âœ… "cleaned record count should never exceed input count"
  âœ… "rejected records + accepted records should equal input count"
  âœ… "all timestamps in cleaned data should be parseable"
  âœ… "all user IDs in cleaned data should be valid"
  âœ… "track keys should be deterministic for identical inputs"
  âœ… "no cleaned record should have null critical fields"
  âœ… "cleaning should be idempotent (cleaning cleaned data = no change)"
  âœ… "temporal ordering should be preserved within user sessions"
}
```

### **Edge Case Scenario Tests**

#### **Real-World Data Challenges**
```scala
class EdgeCaseScenarioTests {
  // Data Volume Edge Cases
  âœ… "should handle users with no listening history"
  âœ… "should handle users with single track play"
  âœ… "should handle users with extremely high play counts (>100k)"
  âœ… "should handle datasets with no valid records"
  âœ… "should handle datasets with all identical records"
  
  // Character Encoding Challenges (Based on analysis results)
  âœ… "should handle Greek artist names (Îœ-Ziq, ÎˆÎ»ÎµÎ½Î± Î Î±Ï€Î±ÏÎ¯Î¶Î¿Ï…)"
  âœ… "should handle Cyrillic artist names (ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¾Ñ‡Ð½Ð°Ñ Ð›Ð¸Ð½ÐµÐ¹ÐºÐ°, Ð›ÐµÐ½Ð¸Ð½Ð° ÐŸÐ°ÐºÐµÑ‚)"
  âœ… "should handle mixed character sets in single record"
  âœ… "should handle corrupted character encoding"
  
  // Timestamp Edge Cases
  âœ… "should handle year 2038 problem (Unix timestamp overflow)"
  âœ… "should handle leap year February 29th timestamps"
  âœ… "should handle timezone transitions (spring forward, fall back)"
  âœ… "should handle historical timezone changes"
  
  // Music Industry Edge Cases
  âœ… "should handle extremely long song titles (>500 characters)"
  âœ… "should handle songs with only punctuation as names"
  âœ… "should handle classical music naming conventions"
  âœ… "should handle podcast episodes vs music tracks"
  âœ… "should handle live performance vs studio recording variants"
}
```

---

## ðŸ“Š **Quality Monitoring & Metrics**

### **Critical Quality Metrics (Must Track)**
- **Processing Success Rate**: % of records successfully cleaned vs rejected
- **Critical Field Validity**: % of records with valid user ID and timestamp
- **Track Identity Confidence**: Distribution of MBID vs name-only matching
- **Timestamp Chronology**: % of users with proper chronological ordering
- **Cleaning Effectiveness**: Before/after quality score comparison

### **Quality Thresholds (Based on ADR-005)**
- **Acceptable**: < 5% rejection rate overall
- **Warning**: 5-10% rejection rate (investigate data source)
- **Critical**: > 10% rejection rate (halt processing)

### **Specific Session Analysis Metrics**
- **Session Boundary Integrity**: % of valid timestamp sequences per user
- **Track Matching Confidence**: 
  - High: Track MBID available (target: maintain >85%)
  - Medium: Artist MBID only (target: <10%)  
  - Low: Name matching only (target: <5%)

---

## ðŸš¨ **Error Handling Strategy**

### **Error Classification System**
```scala
sealed trait CleaningError
case class CriticalFieldError(field: String, value: String, reason: String) extends CleaningError
case class StandardizationWarning(field: String, original: String, cleaned: String) extends CleaningError
case class QualityAlert(metric: String, threshold: Double, actual: Double) extends CleaningError
```

### **Recovery Strategies**
- **Critical Errors**: Reject record, log for investigation, continue processing
- **Standardization Issues**: Apply cleaning rules, log transformation, continue processing
- **Quality Warnings**: Accept record, flag for monitoring, continue processing

### **Alerting Triggers**
- **Immediate**: Critical field error rate > 10%
- **Hourly**: Quality degradation trends
- **Daily**: Comprehensive quality reports

---

## ðŸŽ¯ **Success Criteria**

### **Functional Requirements**
- âœ… All records suitable for session analysis (valid user ID + timestamp)
- âœ… Consistent track identification across 96.9% of data
- âœ… Proper handling of international character sets
- âœ… Deterministic, repeatable cleaning results

### **Performance Requirements**
- âœ… Process 19M records in < 1 minute (cleaning component only)
- âœ… Memory usage < 2GB for cleaning operations
- âœ… Quality report generation < 30 seconds

### **Quality Requirements**
- âœ… Overall rejection rate < 5%
- âœ… Track identity confidence > 95%
- âœ… Zero data corruption during cleaning
- âœ… Complete audit trail of all cleaning operations

---

## ðŸ”— **Implementation Notes**

### **Integration with Existing Architecture**
- **Aligns with ADR-005**: Hybrid Data Cleaning Approach with tiered validation
- **Supports ADR-001**: Hexagonal Architecture through clean separation of validation concerns
- **Enables ADR-003**: Optimal partitioning by ensuring clean userId fields for repartitioning
- **Complements ADR-007**: Comprehensive testing strategy with extensive edge case coverage

### **Pipeline Context Integration**
- **Data Quality Context**: Primary implementation location for cleaning logic
- **Session Analysis Context**: Consumer of cleaned data with quality guarantees
- **Ranking Context**: Benefits from standardized track identity resolution

### **Monitoring Integration**
- **Quality Metrics**: Seamless integration with existing data quality reporting framework
- **Performance Tracking**: Cache efficiency and processing time monitoring
- **Business Impact**: Track cleaning effectiveness impact on session analysis accuracy

---

*Document Version: 1.0*  
*Last Updated: [Current Date]*  
*Author: Data Engineering Team*  
*Review Cycle: Before each major release*


