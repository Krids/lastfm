package com.lastfm.sessions.infrastructure

import com.lastfm.sessions.domain.{DataRepositoryPort, ListenEvent, DataQualityMetrics}
import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import java.time.Instant
import scala.util.{Try, Success, Failure}
import scala.util.control.NonFatal

/**
 * Spark-based implementation of DataRepositoryPort for loading LastFM listening events.
 * 
 * This implementation uses Apache Spark to efficiently process large TSV files containing
 * listening event data. It handles data validation, schema enforcement, and error recovery
 * while maintaining the contract defined by the DataRepositoryPort interface.
 * 
 * Key features:
 * - Efficient TSV parsing with schema validation
 * - Data quality filtering for malformed records
 * - Unicode character support for international content
 * - Memory-efficient processing for large datasets
 * - Comprehensive error handling and recovery
 * 
 * @param spark Implicit SparkSession for data processing operations
 * @author LastFM Session Analysis Team
 * @since 1.0.0
 */
class SparkDataRepository(implicit spark: SparkSession) extends DataRepositoryPort {
  
  import spark.implicits._
  
  /**
   * Schema definition for LastFM TSV files.
   * 
   * Based on the actual LastFM dataset format:
   * userId | timestamp | artistId | artistName | trackId | trackName
   */
  private val schema = StructType(Seq(
    StructField("userId", StringType, nullable = false),
    StructField("timestamp", StringType, nullable = false),  // Will be parsed to timestamp
    StructField("artistId", StringType, nullable = true),
    StructField("artistName", StringType, nullable = false),
    StructField("trackId", StringType, nullable = true),
    StructField("trackName", StringType, nullable = false)
  ))
  
  /**
   * Loads listening events from a TSV file using Spark DataFrame operations.
   * 
   * This method:
   * 1. Reads TSV file with proper schema
   * 2. Validates and filters malformed records
   * 3. Converts to domain objects
   * 4. Returns validated ListenEvent instances
   * 
   * @param dataSourcePath Path to the TSV file containing listening events
   * @return Try containing either List of ListenEvent objects or failure exception
   */
  override def loadListenEvents(dataSourcePath: String): Try[List[ListenEvent]] = {
    try {
      // Read TSV file with schema
      val rawDF = spark.read
        .option("header", "false")  // No header in LastFM dataset
        .option("delimiter", "\t")   // Tab-separated values
        .option("encoding", "UTF-8") // Support Unicode characters
        .option("mode", "PERMISSIVE") // Handle malformed records gracefully
        .schema(schema)
        .csv(dataSourcePath)
      
      // Filter out malformed records and validate data
      val validatedDF = rawDF
        .filter($"userId".isNotNull && $"userId" =!= "")
        .filter($"timestamp".isNotNull && $"timestamp" =!= "")
        .filter($"artistName".isNotNull && $"artistName" =!= "")
        .filter($"trackName".isNotNull && $"trackName" =!= "")
        // Remove records with only whitespace in required fields
        .filter(trim($"userId") =!= "")
        .filter(trim($"artistName") =!= "")
        .filter(trim($"trackName") =!= "")
      
      // Convert to ListenEvent objects
      val events = validatedDF
        .collect()
        .map(row => convertRowToListenEvent(row))
        .filter(_.isSuccess)
        .map(_.get)
        .toList
      
      Success(events)
      
    } catch {
      case NonFatal(exception) =>
        Failure(exception)
    }
  }
  
  /**
   * Converts a Spark DataFrame Row to a ListenEvent domain object.
   * 
   * Handles type conversion, validation, and error recovery for individual records.
   * 
   * @param row Spark DataFrame row containing listening event data
   * @return Try containing either ListenEvent object or conversion failure
   */
  private def convertRowToListenEvent(row: org.apache.spark.sql.Row): Try[ListenEvent] = {
    try {
      val userId = row.getAs[String]("userId")
      val timestampStr = row.getAs[String]("timestamp")
      val artistIdOpt = Option(row.getAs[String]("artistId")).filter(_.nonEmpty)
      val artistName = row.getAs[String]("artistName")
      val trackIdOpt = Option(row.getAs[String]("trackId")).filter(_.nonEmpty)
      val trackName = row.getAs[String]("trackName")
      
      // Parse timestamp - LastFM uses ISO 8601 format
      val timestamp = Instant.parse(timestampStr)
      
      val event = ListenEvent(
        userId = userId,
        timestamp = timestamp,
        artistId = artistIdOpt,
        artistName = artistName,
        trackId = trackIdOpt,
        trackName = trackName,
        trackKey = trackIdOpt.getOrElse(s"$artistName â€” $trackName")
      )
      
      Success(event)
      
    } catch {
      case NonFatal(exception) =>
        // Log the error but don't fail the entire operation
        Failure(exception)
    }
  }

  override def loadWithDataQuality(dataSourcePath: String): Try[(List[ListenEvent], DataQualityMetrics)] = {
    // Simple implementation for now
    val events = loadListenEvents(dataSourcePath).getOrElse(List.empty)
    val metrics = DataQualityMetrics(
      totalRecords = events.length.toLong,
      validRecords = events.length.toLong,
      rejectedRecords = 0L,
      rejectionReasons = Map.empty,
      trackIdCoverage = 0.0,
      suspiciousUsers = 0L
    )
    Success((events, metrics))
  }

  override def cleanAndPersist(inputPath: String, outputPath: String): Try[DataQualityMetrics] = {
    // Simple implementation for now
    Success(DataQualityMetrics(
      totalRecords = 0L,
      validRecords = 0L, 
      rejectedRecords = 0L,
      rejectionReasons = Map.empty,
      trackIdCoverage = 0.0,
      suspiciousUsers = 0L
    ))
  }
}