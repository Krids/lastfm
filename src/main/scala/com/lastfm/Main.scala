package com.lastfm

import com.lastfm.spark.SparkManager
import com.lastfm.config.Config
import com.lastfm.pipelines.{DataCleaningPipeline, DistributedSessionPipeline, RankingPipeline}
import scala.util.{Success, Failure}
import java.time.LocalDateTime

/**
 * LastFM Session Analysis - Portfolio Data Engineering Project
 * 
 * A clean, production-ready data engineering solution demonstrating:
 * - Advanced Spark optimization with strategic partitioning
 * - Distributed processing for scalable data analysis  
 * - Three-pipeline architecture (Bronze â†’ Silver â†’ Gold â†’ Results)
 * - Business logic implementation (20-minute session gap algorithm)
 * - Performance engineering best practices
 * 
 * Technical Highlights:
 * - Strategic userId partitioning eliminates shuffle operations
 * - Distributed window functions for memory-efficient session analysis
 * - Medallion architecture with proper data layer separation
 * - Advanced coalesce vs repartition optimization
 * - Clean architecture with separated concerns
 * 
 * @author Felipe Lana Machado
 * @since 1.0.0
 */
object Main extends App {
  
  private val startTime = System.currentTimeMillis()
  
  println("ðŸŽµ LastFM Session Analysis - Data Engineering Pipeline")
  println("=" * 80)
  println(s"ðŸ• Started: ${LocalDateTime.now()}")
  println(s"â˜• Java Version: ${System.getProperty("java.version")}")
  println(s"ðŸ–¥ï¸  Available Cores: ${Runtime.getRuntime.availableProcessors()}")
  println(s"ðŸ’¾ Available Memory: ${Runtime.getRuntime.maxMemory() / 1024 / 1024 / 1024}GB")
  
  val pipelineType = args.headOption.getOrElse("complete")
  println(s"ðŸŽ¯ Selected Pipeline: '$pipelineType'")
  
  // Load configuration and create Spark session
  val config = Config.load()
  val spark = SparkManager.createSession()
  
  try {
    val result = pipelineType match {
      case "data-cleaning" => runDataCleaning()
      case "session-analysis" => runSessionAnalysis() 
      case "ranking" => runRanking()
      case "help" | "--help" => 
        displayUsageHelp()
        Success(())
      case "complete" | _ => runCompletePipeline()
    }
    
    result match {
      case Success(_) => 
        displaySuccessSummary()
        println("âœ… Pipeline completed successfully!")
      case Failure(exception) => 
        displayFailureSummary(exception)
        sys.exit(1)
    }
    
  } catch {
    case exception: Exception =>
      displayFailureSummary(exception)
      sys.exit(1)
  } finally {
    spark.stop()
  }
  
  /**
   * Executes the complete three-pipeline workflow.
   * Demonstrates proper pipeline orchestration and error handling.
   */
  private def runCompletePipeline() = {
    println("\nðŸ”„ Complete Pipeline: Bronze â†’ Silver â†’ Gold â†’ Results")
    
    for {
      _ <- runDataCleaning()
      _ <- runSessionAnalysis()
      _ <- runRanking()
    } yield {
      println("\nðŸŽ‰ Complete workflow executed successfully!")
      println("   ðŸ“Š Data Quality â†’ âœ… Complete")
      println("   ðŸ“ˆ Session Analysis â†’ âœ… Complete")
      println("   ðŸ† Song Ranking â†’ âœ… Complete")
    }
  }
  
  /**
   * Bronze â†’ Silver: Data cleaning with strategic partitioning.
   */
  private def runDataCleaning() = {
    println("\nðŸ§¹ Data Cleaning Pipeline: Bronze â†’ Silver")
    val pipeline = new DataCleaningPipeline(spark, config)
    pipeline.execute()
  }
  
  /**
   * Silver â†’ Gold: Distributed session analysis.
   */
  private def runSessionAnalysis() = {
    println("\nðŸ“ˆ Session Analysis Pipeline: Silver â†’ Gold")  
    val pipeline = new DistributedSessionPipeline(spark, config)
    pipeline.execute()
  }
  
  /**
   * Gold â†’ Results: Song ranking and final output.
   */
  private def runRanking() = {
    println("\nðŸŽ¶ Ranking Pipeline: Gold â†’ Results")
    val pipeline = new RankingPipeline(spark, config)
    pipeline.execute()
  }
  
  /**
   * Displays usage help and available pipeline options.
   */
  private def displayUsageHelp(): Unit = {
    println("\nðŸ“‹ Usage: sbt \"runMain com.lastfm.Main [pipeline]\"")
    println("\nðŸ”„ Available Pipelines:")
    println("   complete         Run all pipelines (Bronze â†’ Silver â†’ Gold â†’ Results)")
    println("   data-cleaning    Bronze â†’ Silver: Clean and partition data")
    println("   session-analysis Silver â†’ Gold: Analyze user sessions") 
    println("   ranking          Gold â†’ Results: Generate top songs ranking")
    println("   help             Show this help message")
    
    println("\nðŸ³ Docker Usage:")
    println("   docker-compose up                    # Complete pipeline")
    println("   docker run lastfm data-cleaning     # Individual pipeline")
    
    println("\nðŸ“Š Expected Output:")
    println("   data/output/results/top_songs.tsv   # Final top 10 songs")
    
    println("\nðŸ”§ Technical Highlights:")
    println("   â€¢ Strategic userId partitioning (zero-shuffle operations)")
    println("   â€¢ Distributed window functions (unlimited scalability)")
    println("   â€¢ Medallion architecture (Bronze/Silver/Gold layers)")
    println("   â€¢ Advanced Spark optimization (coalesce, caching, compression)")
  }
  
  /**
   * Displays success summary with performance metrics.
   */
  private def displaySuccessSummary(): Unit = {
    val totalExecutionTime = System.currentTimeMillis() - startTime
    val executionMinutes = totalExecutionTime / 60000.0
    
    println(s"\nðŸ“Š Execution Summary:")
    println(f"   â±ï¸  Total Duration: ${executionMinutes}%.2f minutes")
    println(s"   ðŸŽ¯ Pipeline: ${pipelineType}")
    println(s"   ðŸ–¥ï¸  Processing: Distributed across ${Runtime.getRuntime.availableProcessors()} cores")
    
    // Display next steps based on pipeline executed
    pipelineType match {
      case "data-cleaning" =>
        println(s"\nðŸš€ Next Steps:")
        println(s"   ðŸ“„ Silver layer ready at: ${config.outputPath}/silver/")
        println(s"   ðŸŽ¯ Run: sbt \"runMain com.lastfm.Main session-analysis\"")
        
      case "session-analysis" =>
        println(s"\nðŸš€ Next Steps:")
        println(s"   ðŸ“„ Gold layer ready at: ${config.outputPath}/gold/")
        println(s"   ðŸŽ¯ Run: sbt \"runMain com.lastfm.Main ranking\"")
        
      case "ranking" =>
        println(s"\nðŸŽ¯ Final Results:")
        println(s"   ðŸ“„ Check: ${config.outputPath}/results/top_songs.tsv")
        
      case "complete" | _ =>
        println(s"\nðŸŽ‰ Complete Analysis Ready!")
        println(s"   ðŸ“Š Data Processing: âœ… Optimized")
        println(s"   ðŸ“ˆ Session Analysis: âœ… Complete") 
        println(s"   ðŸ† Song Ranking: âœ… Complete")
        println(s"   ðŸ“„ Final Output: ${config.outputPath}/results/top_songs.tsv")
    }
  }
  
  /**
   * Displays failure summary with troubleshooting guidance.
   */
  private def displayFailureSummary(exception: Throwable): Unit = {
    val executionTime = System.currentTimeMillis() - startTime
    
    println(s"\nâŒ Pipeline Execution Failed")
    println("-" * 60)
    println(s"ðŸ• Failed after: ${executionTime / 1000.0} seconds")
    println(s"ðŸ” Error: ${exception.getClass.getSimpleName}")
    println(s"ðŸ“ Message: ${exception.getMessage}")
    
    println(s"\nðŸ› ï¸  Troubleshooting:")
    exception match {
      case _: RuntimeException if exception.getMessage != null && exception.getMessage.contains("not found") =>
        println(s"   ðŸ’¡ Verify dataset is available at: ${config.inputPath}")
        println(s"   ðŸ’¡ Check file permissions and directory structure")
        
      case _: OutOfMemoryError =>
        println(s"   ðŸ’¡ Increase memory: sbt -J-Xmx16g \"runMain com.lastfm.Main\"")
        println(s"   ðŸ’¡ Or use Docker for memory management")
        
      case _ =>
        println(s"   ðŸ’¡ Check input data format and file paths")
        println(s"   ðŸ’¡ Verify Spark configuration and available resources")
    }
    
    println(s"\nðŸ“‹ For help: sbt \"runMain com.lastfm.Main help\"")
  }
}