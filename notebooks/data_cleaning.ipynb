{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66755717-9890-40ff-a228-2dd91d5c6a0f",
   "metadata": {},
   "source": [
    "# Last.fm — Data Quality Notebook (Scala + Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea62fad-bd22-4dba-827b-7e8e2a4bda80",
   "metadata": {},
   "source": [
    "**Goal:** Manually verify (and document) data quality for the Last.fm 1K users dataset using **Scala + Spark**.\n",
    "\n",
    "**What this notebook covers:**\n",
    "1. Ingestion with **explicit schema** and **UTC timezone**.\n",
    "2. **Robust timestamp parsing** and counting invalid rows.\n",
    "3. **Key normalization**: prefer `track_id` (MBID), fallback to `artist_name — track_name`.\n",
    "4. **String sanitization** (trim, remove control chars).\n",
    "5. **Data quality metrics** (read/valid/dropped rows, % missing MBIDs).\n",
    "6. **Policy for empty fields** (user_id / artist_name / track_name).\n",
    "7. **Semantic rule checks** for session gaps: **= 20 min** vs **> 20 min**.\n",
    "8. **Duplicate detection** (same user, same timestamp, same track).\n",
    "9. *(Optional)* **Deequ** constraints (nullability, uniqueness).\n",
    "\n",
    "**Tested/compatible with:** Scala **2.12** and Spark **3.5.x**. Adjust library coordinates accordingly if you add optional Deequ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eda3bd9-9d9b-4580-8204-599edd2244eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.52.2.min',\n",
       "    jquery: 'https://code.jquery.com/jquery-3.3.1.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using INPUT_PATH  = /Users/Felipe/lastfm/data/lastfm/lastfm-dataset-1k/userid-timestamp-artid-artname-traid-traname.tsv\n",
      "Using PROFILE_PATH = /Users/Felipe/lastfm/data/lastfm/lastfm-dataset-1k/userid-profile.tsv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._, plotly.element._, plotly.layout._, plotly.Almond._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{SparkSession, DataFrame}\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.logging.log4j.{LogManager, Level => LogLevel}\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.logging.log4j.core.Logger\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\u001b[39m\n",
       "\u001b[36mres2_9\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"WARN\"\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@74578c3\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m\n",
       "\u001b[36mINPUT_PATH\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/Users/Felipe/lastfm/data/lastfm/lastfm-dataset-1k/userid-timestamp-artid-artname-traid-traname.tsv\"\u001b[39m\n",
       "\u001b[36mPROFILE_PATH\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/Users/Felipe/lastfm/data/lastfm/lastfm-dataset-1k/userid-profile.tsv\"\u001b[39m\n",
       "\u001b[36mSAMPLE_ROWS\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m20\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.1`\n",
    "import $ivy.`org.plotly-scala::plotly-almond:0.8.0`\n",
    "import plotly._, plotly.element._, plotly.layout._, plotly.Almond._\n",
    "init()\n",
    "\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.logging.log4j.{LogManager, Level => LogLevel}\n",
    "import org.apache.logging.log4j.core.Logger\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// Ajusta o nível de log para suprimir INFO antes de iniciar o Spark\n",
    "System.setProperty(\"log4j2.level\", \"WARN\")\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"LastFM-DataCleaning\")\n",
    "  .master(\"local[*]\")\n",
    "  .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "  .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "// Reduz log para ERROR em loggers Spark e Hadoop\n",
    "Seq(\n",
    "  \"org.apache.spark\",\n",
    "  \"org.apache.spark.sql.execution\",\n",
    "  \"org.apache.spark.storage\",\n",
    "  \"org.apache.hadoop\",\n",
    "  \"org.spark_project\"\n",
    ").foreach { name =>\n",
    "  LogManager.getLogger(name).asInstanceOf[Logger].setLevel(LogLevel.ERROR)\n",
    "}\n",
    "\n",
    "LogManager.getRootLogger.asInstanceOf[Logger].setLevel(LogLevel.ERROR)\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "// Defaults\n",
    "var INPUT_PATH: String = \"/Users/Felipe/lastfm/data/lastfm/lastfm-dataset-1k/userid-timestamp-artid-artname-traid-traname.tsv\" // main play logs TSV\n",
    "var PROFILE_PATH: String = \"/Users/Felipe/lastfm/data/lastfm/lastfm-dataset-1k/userid-profile.tsv\"\n",
    "val SAMPLE_ROWS = 20 // number of rows to show in samples\n",
    "\n",
    "println(s\"Using INPUT_PATH  = ${INPUT_PATH}\")\n",
    "println(s\"Using PROFILE_PATH = ${PROFILE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad9d231-468b-46cd-b706-a4996b0c99d1",
   "metadata": {},
   "source": [
    "## 1) Ingestion with explicit schema\n",
    "**Purpose:** Avoid incorrect type inference and lock expected column order.\n",
    "\n",
    "**Columns:**\n",
    " - `user_id` (String, not null)\n",
    " - `ts_str` (String, not null) — raw timestamp to be parsed later\n",
    " - `artist_id` (String, nullable) — MBID\n",
    " - `artist_name` (String, nullable)\n",
    " - `track_id` (String, nullable) — MBID\n",
    " - `track_name` (String, nullable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3671a2a-3dbb-40e9-b577-d1f34362b24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd3.sc:16: Auto-application to `()` is deprecated. Supply the empty argument list `()` explicitly to invoke method count,\n",
      "or remove the empty argument list from its definition (Java-defined methods are exempt).\n",
      "In Scala 3, an unapplied method like this will be eta-expanded into a function. [quickfixable]\n",
      "val rowsRead = rawDf.count\n",
      "                     ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows read (raw): 19150868\n",
      "+-----------+--------------------+------------------------------------+---------------+------------------------------------+------------------------------------------+\n",
      "|user_id    |ts_str              |artist_id                           |artist_name    |track_id                            |track_name                                |\n",
      "+-----------+--------------------+------------------------------------+---------------+------------------------------------+------------------------------------------+\n",
      "|user_000001|2009-05-04T23:08:57Z|f1b1cf71-bd35-4e99-8624-24a6e15f133a|Deep Dish      |NULL                                |Fuck Me Im Famous (Pacha Ibiza)-09-28-2007|\n",
      "|user_000001|2009-05-04T13:54:10Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Composition 0919 (Live_2009_4_15)         |\n",
      "|user_000001|2009-05-04T13:52:04Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Mc2 (Live_2009_4_15)                      |\n",
      "|user_000001|2009-05-04T13:42:52Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Hibari (Live_2009_4_15)                   |\n",
      "|user_000001|2009-05-04T13:42:11Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Mc1 (Live_2009_4_15)                      |\n",
      "|user_000001|2009-05-04T13:38:31Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |To Stanford (Live_2009_4_15)              |\n",
      "|user_000001|2009-05-04T13:33:28Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Improvisation (Live_2009_4_15)            |\n",
      "|user_000001|2009-05-04T13:23:45Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Glacier (Live_2009_4_15)                  |\n",
      "|user_000001|2009-05-04T13:19:22Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Parolibre (Live_2009_4_15)                |\n",
      "|user_000001|2009-05-04T13:13:38Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Bibo No Aozora (Live_2009_4_15)           |\n",
      "|user_000001|2009-05-04T13:06:09Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |f7c1f8f8-b935-45ed-8fc8-7def69d92a10|The Last Emperor (Theme)                  |\n",
      "|user_000001|2009-05-04T13:00:48Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Happyend (Live_2009_4_15)                 |\n",
      "|user_000001|2009-05-04T12:55:34Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |475d4e50-cebb-4cd0-8cd4-c3df97987962|Tibetan Dance (Version)                   |\n",
      "|user_000001|2009-05-04T12:51:26Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Behind The Mask (Live_2009_4_15)          |\n",
      "|user_000001|2009-05-03T15:48:25Z|ba2f4f3b-0293-4bc8-bb94-2f73b5207343|Underworld     |dc394163-2b78-4b56-94e4-658597a29ef8|Boy, Boy, Boy (Switch Remix)              |\n",
      "|user_000001|2009-05-03T15:37:56Z|ba2f4f3b-0293-4bc8-bb94-2f73b5207343|Underworld     |340d9a0b-9a43-4098-b116-9f79811bd508|Crocodile (Innervisions Orchestra Mix)    |\n",
      "|user_000001|2009-05-03T15:14:53Z|a16e47f5-aa54-47fe-87e4-bb8af91a9fdd|Ennio Morricone|0b04407b-f517-4e00-9e6a-494795efc73e|Ninna Nanna In Blu (Raw Deal Remix)       |\n",
      "|user_000001|2009-05-03T15:10:18Z|463a94f1-2713-40b1-9c88-dcc9c0170cae|Minus 8        |4e78efc4-e545-47af-9617-05ff816d86e2|Elysian Fields                            |\n",
      "|user_000001|2009-05-03T15:04:31Z|ad0811ea-e213-451d-b22f-fa1a7f9e0226|Beanfield      |fb51d2c4-cc69-4128-92f5-77ec38d66859|Planetary Deadlock                        |\n",
      "|user_000001|2009-05-03T14:56:25Z|309e2dfc-678e-4d09-a7a4-8eab9525b669|Dj Linus       |4277434f-e3c2-41ae-9ce3-23fd157f9347|Good Morning Love Coffee Is Ready         |\n",
      "+-----------+--------------------+------------------------------------+---------------+------------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- ts_str: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mschema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mSeq\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"user_id\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mfalse\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"ts_str\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mfalse\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"artist_id\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"artist_name\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"track_id\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"track_name\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  )\n",
       ")\n",
       "\u001b[36mrawDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_id: string, ts_str: string ... 4 more fields]\n",
       "\u001b[36mrowsRead\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m19150868L\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = StructType(Seq(\n",
    "  StructField(\"user_id\", StringType, nullable = false),\n",
    "  StructField(\"ts_str\", StringType, nullable = false),\n",
    "  StructField(\"artist_id\", StringType, nullable = true),\n",
    "  StructField(\"artist_name\", StringType, nullable = true),\n",
    "  StructField(\"track_id\", StringType, nullable = true),\n",
    "  StructField(\"track_name\", StringType, nullable = true)\n",
    "))\n",
    "\n",
    "val rawDf = spark.read\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .option(\"header\", \"false\")\n",
    "  .schema(schema)\n",
    "  .csv(INPUT_PATH)\n",
    "\n",
    "val rowsRead = rawDf.count\n",
    "println(s\"Rows read (raw): ${rowsRead}\")\n",
    "rawDf.show(SAMPLE_ROWS, truncate = false)\n",
    "rawDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5366da0-54a9-4320-a368-8244409586e0",
   "metadata": {},
   "source": [
    "## 2) Robust timestamp parsing & invalid row accounting\n",
    "**Purpose:** Parse `ts_str` into a proper `timestamp` column (`ts`) and count invalid rows. Keep only rows with a valid timestamp.\n",
    "\n",
    "**Format used:** `yyyy-MM-dd'T'HH:mm:ss` (UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f45db9-b2ce-421a-a112-d40f25ad44fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:58:45.145 [Executor task launch worker for task 4.0 in stage 5.0 (TID 41)] ERROR org.apache.spark.executor.Executor - Exception in task 4.0 in stage 5.0 (TID 41)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2008-09-27T06:42:54Z' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35) ~[scala-library-2.13.12.jar:?]\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43) ~[spark-sql_2.13-3.5.1.jar:3.5.1]\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583) ~[scala-library-2.13.12.jar:?]\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
      "\tat java.lang.Thread.run(Thread.java:829) [?:?]\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2008-09-27T06:42:54Z' could not be parsed, unparsed text found at index 19\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049) ~[?:?]\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874) ~[?:?]\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\t... 19 more\n",
      "17:58:45.146 [Executor task launch worker for task 8.0 in stage 5.0 (TID 45)] ERROR org.apache.spark.executor.Executor - Exception in task 8.0 in stage 5.0 (TID 45)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2008-03-21T10:49:32Z' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35) ~[scala-library-2.13.12.jar:?]\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43) ~[spark-sql_2.13-3.5.1.jar:3.5.1]\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583) ~[scala-library-2.13.12.jar:?]\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
      "\tat java.lang.Thread.run(Thread.java:829) [?:?]\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2008-03-21T10:49:32Z' could not be parsed, unparsed text found at index 19\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049) ~[?:?]\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874) ~[?:?]\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\t... 19 more\n",
      "17:58:45.146 [Executor task launch worker for task 0.0 in stage 5.0 (TID 37)] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 5.0 (TID 37)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2009-05-04T23:08:57Z' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35) ~[scala-library-2.13.12.jar:?]\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43) ~[spark-sql_2.13-3.5.1.jar:3.5.1]\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583) ~[scala-library-2.13.12.jar:?]\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
      "\tat java.lang.Thread.run(Thread.java:829) [?:?]\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2009-05-04T23:08:57Z' could not be parsed, unparsed text found at index 19\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049) ~[?:?]\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874) ~[?:?]\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\t... 19 more\n",
      "17:58:45.150 [task-result-getter-3] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 4.0 in stage 5.0 (TID 41) (192.168.88.39 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2008-09-27T06:42:54Z' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2008-09-27T06:42:54Z' could not be parsed, unparsed text found at index 19\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n",
      "\t... 19 more\n",
      "\n",
      "17:58:45.145 [Executor task launch worker for task 11.0 in stage 5.0 (TID 48)] ERROR org.apache.spark.executor.Executor - Exception in task 11.0 in stage 5.0 (TID 48)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2008-10-04T18:40:09Z' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35) ~[scala-library-2.13.12.jar:?]\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43) ~[spark-sql_2.13-3.5.1.jar:3.5.1]\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583) ~[scala-library-2.13.12.jar:?]\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.13-3.5.1.jar:3.5.1]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
      "\tat java.lang.Thread.run(Thread.java:829) [?:?]\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2008-10-04T18:40:09Z' could not be parsed, unparsed text found at index 19\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049) ~[?:?]\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874) ~[?:?]\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193) ~[spark-sql-api_2.13-3.5.1.jar:3.5.1]\n",
      "\t... 19 more\n",
      "17:58:45.152 [task-result-getter-3] ERROR org.apache.spark.scheduler.TaskSetManager - Task 4 in stage 5.0 failed 1 times; aborting job\n",
      "17:58:45.155 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 5.0 (TID 37) (192.168.88.39 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2009-05-04T23:08:57Z' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": "Job aborted due to stage failure: Task 4 in stage 5.0 failed 1 times, most recent failure: Lost task 4.0 in stage 5.0 (TID 41) (192.168.88.39 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2008-09-27T06:42:54Z' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.time.format.DateTimeParseException: Text '2008-09-27T06:42:54Z' could not be parsed, unparsed text found at index 19\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n\t... 19 more\n\nDriver stacktrace:",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 5.0 failed 1 times, most recent failure: Lost task 4.0 in stage 5.0 (TID 41) (192.168.88.39 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2008-09-27T06:42:54Z' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.time.format.DateTimeParseException: Text '2008-09-27T06:42:54Z' could not be parsed, unparsed text found at index 19\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n\t... 19 more\n\nDriver stacktrace:\u001b[39m",
      "  org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2856\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2792\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2791\u001b[39m)",
      "  scala.collection.immutable.List.foreach(\u001b[32mList.scala\u001b[39m:\u001b[32m333\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2791\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1247\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1247\u001b[39m)",
      "  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m437\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1247\u001b[39m)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m3060\u001b[39m)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2994\u001b[39m)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2983\u001b[39m)",
      "  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m49\u001b[39m)",
      "\u001b[31morg.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2008-09-27T06:42:54Z' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\u001b[39m",
      "  org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(\u001b[32mExecutionErrors.scala\u001b[39m:\u001b[32m54\u001b[39m)",
      "  org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(\u001b[32mExecutionErrors.scala\u001b[39m:\u001b[32m48\u001b[39m)",
      "  org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(\u001b[32mExecutionErrors.scala\u001b[39m:\u001b[32m218\u001b[39m)",
      "  org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(\u001b[32mDateTimeFormatterHelper.scala\u001b[39m:\u001b[32m142\u001b[39m)",
      "  org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(\u001b[32mDateTimeFormatterHelper.scala\u001b[39m:\u001b[32m135\u001b[39m)",
      "  scala.runtime.AbstractPartialFunction.apply(\u001b[32mAbstractPartialFunction.scala\u001b[39m:\u001b[32m35\u001b[39m)",
      "  org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(\u001b[32mTimestampFormatter.scala\u001b[39m:\u001b[32m195\u001b[39m)",
      "  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(\u001b[32mUnknown Source\u001b[39m)",
      "  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(\u001b[32mUnknown Source\u001b[39m)",
      "  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(\u001b[32mBufferedRowIterator.java\u001b[39m:\u001b[32m43\u001b[39m)",
      "  org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(\u001b[32mWholeStageCodegenEvaluatorFactory.scala\u001b[39m:\u001b[32m43\u001b[39m)",
      "  scala.collection.Iterator$$anon$9.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m583\u001b[39m)",
      "  org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(\u001b[32mBypassMergeSortShuffleWriter.java\u001b[39m:\u001b[32m140\u001b[39m)",
      "  org.apache.spark.shuffle.ShuffleWriteProcessor.write(\u001b[32mShuffleWriteProcessor.scala\u001b[39m:\u001b[32m59\u001b[39m)",
      "  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m104\u001b[39m)",
      "  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m54\u001b[39m)",
      "  org.apache.spark.TaskContext.runTaskWithListeners(\u001b[32mTaskContext.scala\u001b[39m:\u001b[32m166\u001b[39m)",
      "  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m141\u001b[39m)",
      "  org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m620\u001b[39m)",
      "  org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(\u001b[32mSparkErrorUtils.scala\u001b[39m:\u001b[32m64\u001b[39m)",
      "  org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(\u001b[32mSparkErrorUtils.scala\u001b[39m:\u001b[32m61\u001b[39m)",
      "  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m94\u001b[39m)",
      "  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m623\u001b[39m)",
      "  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1128\u001b[39m)",
      "  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m628\u001b[39m)",
      "  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m829\u001b[39m)",
      "\u001b[31mjava.time.format.DateTimeParseException: Text '2008-09-27T06:42:54Z' could not be parsed, unparsed text found at index 19\u001b[39m",
      "  java.time.format.DateTimeFormatter.parseResolved0(\u001b[32mDateTimeFormatter.java\u001b[39m:\u001b[32m2049\u001b[39m)",
      "  java.time.format.DateTimeFormatter.parse(\u001b[32mDateTimeFormatter.java\u001b[39m:\u001b[32m1874\u001b[39m)",
      "  org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(\u001b[32mTimestampFormatter.scala\u001b[39m:\u001b[32m193\u001b[39m)",
      "  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(\u001b[32mUnknown Source\u001b[39m)",
      "  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(\u001b[32mUnknown Source\u001b[39m)",
      "  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(\u001b[32mBufferedRowIterator.java\u001b[39m:\u001b[32m43\u001b[39m)",
      "  org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(\u001b[32mWholeStageCodegenEvaluatorFactory.scala\u001b[39m:\u001b[32m43\u001b[39m)",
      "  scala.collection.Iterator$$anon$9.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m583\u001b[39m)",
      "  org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(\u001b[32mBypassMergeSortShuffleWriter.java\u001b[39m:\u001b[32m140\u001b[39m)",
      "  org.apache.spark.shuffle.ShuffleWriteProcessor.write(\u001b[32mShuffleWriteProcessor.scala\u001b[39m:\u001b[32m59\u001b[39m)",
      "  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m104\u001b[39m)",
      "  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m54\u001b[39m)",
      "  org.apache.spark.TaskContext.runTaskWithListeners(\u001b[32mTaskContext.scala\u001b[39m:\u001b[32m166\u001b[39m)",
      "  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m141\u001b[39m)",
      "  org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m620\u001b[39m)",
      "  org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(\u001b[32mSparkErrorUtils.scala\u001b[39m:\u001b[32m64\u001b[39m)",
      "  org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(\u001b[32mSparkErrorUtils.scala\u001b[39m:\u001b[32m61\u001b[39m)",
      "  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m94\u001b[39m)",
      "  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m623\u001b[39m)",
      "  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1128\u001b[39m)",
      "  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m628\u001b[39m)",
      "  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m829\u001b[39m)"
     ]
    }
   ],
   "source": [
    "val DT_FMT = \"yyyy-MM-dd'T'HH:mm:ss\"\n",
    "\n",
    "val withTsDf = rawDf\n",
    "  .withColumn(\"ts\", to_timestamp(col(\"ts_str\"), DT_FMT))\n",
    "  .drop(\"ts_str\")\n",
    "\n",
    "val invalidTsCount = withTsDf.filter(col(\"ts\").isNull).count()\n",
    "val validTsDf = withTsDf.filter(col(\"ts\").isNotNull)\n",
    "\n",
    "println(s\"Invalid due to timestamp parse: ${invalidTsCount}\")\n",
    "println(s\"Valid rows after ts parse: ${validTsDf.count()}\")\n",
    "\n",
    "validTsDf.show(SAMPLE_ROWS, truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07118f84-37f5-4ce9-8f3a-84968ca905c4",
   "metadata": {},
   "source": [
    "## 3) Key normalization (MBID preferred; fallback to `artist_name — track_name`)\n",
    "**Purpose:** Build a stable `track_key` used for counts/joins even when MBIDs are missing.\n",
    "\n",
    "**Rule:** If `track_id` (MBID) is present & non-empty, use it; otherwise use `${artist_name} — ${track_name}` with nulls replaced by `?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b331dc4-91f5-425a-bffd-f27f684f3ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val normalizedDf = validTsDf.withColumn(\n",
    "  \"track_key\",\n",
    "  when(col(\"track_id\").isNotNull && length(col(\"track_id\")) > 0, col(\"track_id\"))\n",
    "    .otherwise(concat_ws(\" — \", coalesce(col(\"artist_name\"), lit(\"?\")), coalesce(col(\"track_name\"), lit(\"?\"))))\n",
    ")\n",
    "\n",
    "normalizedDf.select(\"user_id\",\"ts\",\"artist_id\",\"artist_name\",\"track_id\",\"track_name\",\"track_key\")\n",
    "  .show(SAMPLE_ROWS, truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c7a759-d83a-4eb6-b97f-a6f9b62a89d9",
   "metadata": {},
   "source": [
    "## 4) String sanitization\n",
    "**Purpose:** Remove control characters and trim whitespace from key string fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae741807-93a3-4ce7-b321-1246019f7bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val sanitizeUdf = udf { s: String =>\n",
    "  if (s == null) null\n",
    "  else s.replaceAll(\"\\\\p{Cntrl}\", \"\").trim\n",
    "}\n",
    "\n",
    "val cleanDf = normalizedDf\n",
    "  .withColumn(\"artist_name\", sanitizeUdf(col(\"artist_name\")))\n",
    "  .withColumn(\"track_name\", sanitizeUdf(col(\"track_name\")))\n",
    "  .withColumn(\"user_id\", sanitizeUdf(col(\"user_id\")))\n",
    "\n",
    "cleanDf.select(\"user_id\",\"artist_name\",\"track_name\").show(SAMPLE_ROWS, truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29afadfd-aa24-4c65-bbcc-7a81c4c019fc",
   "metadata": {},
   "source": [
    "## 5) Data Quality metrics summary\n",
    "**Purpose:** Summarize read/valid/dropped counts and percent of missing MBIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809b0cb-3481-4e09-8dd5-4c99583c94e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val totalRead = rowsRead\n",
    "val totalValid = cleanDf.count()\n",
    "val totalDropped = totalRead - totalValid\n",
    "val missingTrackId = cleanDf.filter(col(\"track_id\").isNull || length(col(\"track_id\")) === 0).count()\n",
    "val pctMissingTrackId = if (totalValid == 0) 0.0 else missingTrackId.toDouble / totalValid * 100.0\n",
    "\n",
    "println(f\"rows_read            : ${totalRead}%d\")\n",
    "println(f\"rows_valid           : ${totalValid}%d\")\n",
    "println(f\"rows_dropped         : ${totalDropped}%d\")\n",
    "println(f\"missing_track_id     : ${missingTrackId}%d\")\n",
    "println(f\"pct_missing_track_id : ${pctMissingTrackId}%.2f%%\")\n",
    "\n",
    "Seq(\n",
    "  (\"rows_read\", totalRead.toString),\n",
    "  (\"rows_valid\", totalValid.toString),\n",
    "  (\"rows_dropped\", totalDropped.toString),\n",
    "  (\"missing_track_id\", missingTrackId.toString),\n",
    "  (\"pct_missing_track_id\", f\"${pctMissingTrackId}%.2f%%\")\n",
    ").toDF(\"metric\",\"value\").show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019806a3-e77b-48dc-8550-3189058655c2",
   "metadata": {},
   "source": [
    "## 6) Empty-field policies\n",
    "**Purpose:** Inspect how many rows have empty `user_id`, `artist_name`, or `track_name` and decide whether to drop or impute.\n",
    "\n",
    "**Recommendation:** Drop rows with empty `user_id` and either drop or mark unknown `artist/track` depending on downstream needs (document in README).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e291dc-d97b-44b0-bba8-2606a65c5b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "val emptyUser = cleanDf.filter(coalesce(col(\"user_id\"), lit(\"\")) === \"\").count()\n",
    "val emptyArtist = cleanDf.filter(coalesce(col(\"artist_name\"), lit(\"\")) === \"\").count()\n",
    "val emptyTrack = cleanDf.filter(coalesce(col(\"track_name\"), lit(\"\")) === \"\").count()\n",
    "\n",
    "println(s\"Rows with empty user_id     : $emptyUser\")\n",
    "println(s\"Rows with empty artist_name : $emptyArtist\")\n",
    "println(s\"Rows with empty track_name  : $emptyTrack\")\n",
    "\n",
    "val DROP_EMPTY = true // toggle this policy if needed\n",
    "val dqDf = if (DROP_EMPTY) {\n",
    "  cleanDf.filter(col(\"user_id\") =!= \"\" && col(\"artist_name\") =!= \"\" && col(\"track_name\") =!= \"\")\n",
    "} else cleanDf\n",
    "\n",
    "println(s\"Rows after empty-field policy: ${dqDf.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a109a372-684a-40f3-8e20-a65279aebf36",
   "metadata": {},
   "source": [
    "## 7) Semantic rule — session gap boundary (=20 vs >20 minutes)\n",
    "**Purpose:** Build a tiny synthetic dataset to verify the session split rule at 20 minutes.\n",
    "\n",
    "**Rule:**\n",
    "- gap **≤ 20** minutes → same session\n",
    "- gap **> 20** minutes → new session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8a466-46d7-4ed3-96d4-89b8e7a5cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.sql.Timestamp\n",
    "\n",
    "def ts(s: String) = Timestamp.valueOf(s.replace(\"T\", \" \"))\n",
    "\n",
    "val sessionCheck = Seq(\n",
    "  (\"u1\", ts(\"2023-01-01T10:00:00\"), \"A\"),\n",
    "  (\"u1\", ts(\"2023-01-01T10:20:00\"), \"B\"), // exactly 20 min → SAME session\n",
    "  (\"u1\", ts(\"2023-01-01T10:40:01\"), \"C\"), // > 20 min → NEW session\n",
    "  (\"u2\", ts(\"2023-01-01T09:00:00\"), \"X\"),\n",
    "  (\"u2\", ts(\"2023-01-01T09:15:00\"), \"Y\")  // < 20 min → SAME session\n",
    ").toDF(\"user_id\",\"ts\",\"track_key\")\n",
    "\n",
    "sessionCheck.orderBy(\"user_id\",\"ts\").show(truncate = false)\n",
    "\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "val w = Window.partitionBy(\"user_id\").orderBy(col(\"ts\").asc)\n",
    "\n",
    "val prevTs = lag(col(\"ts\"), 1).over(w)\n",
    "val gapSec = (col(\"ts\").cast(\"long\") - prevTs.cast(\"long\"))\n",
    "val gapMin = when(prevTs.isNull, lit(null).cast(\"double\")).otherwise(gapSec / 60.0)\n",
    "\n",
    "val withGaps = sessionCheck\n",
    "  .withColumn(\"prev_ts\", prevTs)\n",
    "  .withColumn(\"gap_minutes\", gapMin)\n",
    "  .withColumn(\"is_new_session\", when(prevTs.isNull, 1).when(col(\"gap_minutes\") > 20.0, 1).otherwise(0))\n",
    "  .withColumn(\"session_seq\", sum(col(\"is_new_session\")).over(w))\n",
    "\n",
    "withGaps.orderBy(\"user_id\",\"ts\").show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f54b8dd-1b73-4569-97e8-b49b2a840ea8",
   "metadata": {},
   "source": [
    "## 8) Duplicate detection\n",
    "**Purpose:** Identify potential duplicates defined as **same user, same timestamp, same track**.\n",
    "\n",
    "**Action:** Count duplicates and preview a few; decide whether to drop or keep (document in README)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f91527-90e1-447a-bd5b-28d6946acd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "val dupCols = Seq(\"user_id\",\"ts\",\"track_key\")\n",
    "\n",
    "val dupCounts = dqDf\n",
    "  .groupBy(dupCols.map(col): _*)\n",
    "  .agg(count(lit(1)).alias(\"cnt\"))\n",
    "  .filter(col(\"cnt\") > 1)\n",
    "\n",
    "val totalDupRows = if (dupCounts.head(1).isEmpty) 0L else dupCounts.select(sum(\"cnt\")).first.getLong(0)\n",
    "\n",
    "println(s\"Distinct duplicate keys: ${dupCounts.count()}\")\n",
    "println(s\"Total duplicated rows   : ${totalDupRows}\")\n",
    "\n",
    "dupCounts.orderBy(col(\"cnt\").desc).show(20, truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e42caf2-e639-4207-8c38-e53b74e7afe9",
   "metadata": {},
   "source": [
    "## 9) (Optional) Deequ constraints\n",
    "**Purpose:** Validate constraints like nullability and uniqueness using **AWS Deequ**. This section is optional and requires adding Deequ as a dependency.\n",
    "\n",
    "**How to enable:**\n",
    "- Add library: `\"com.amazon.deequ\" %% \"deequ\" % \"2.0.7-spark-3.3\"` (or a version compatible with your Spark).\n",
    "- Then run checks such as: not-null on `user_id`, timestamp; uniqueness on `(user_id, ts)` if required.\n",
    "\n",
    "**Note:** The exact coordinates vary by Spark/Scala versions; confirm compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d449233-54a2-4e88-906d-f3c2ee1cd506",
   "metadata": {},
   "source": [
    "## 10) Summary & Next steps\n",
    "**What we verified:**\n",
    "- Explicit schema & UTC timezone.\n",
    "- Timestamp parsing with invalid-row accounting.\n",
    "- Track key normalization (MBID preferred, fallback safe).\n",
    "- String sanitization.\n",
    "- DQ metrics (read/valid/dropped, % missing MBIDs).\n",
    "- Empty-field policies and their impact.\n",
    "- Semantic rule at the 20-minute boundary (session split).\n",
    "- Duplicate detection and preview.\n",
    " \n",
    "**Next steps (suggested):**\n",
    "- Decide and enforce final policies (drop vs. impute) and document in README.\n",
    "- Persist cleaned datasets to a curated zone (e.g., Parquet, partitioned).\n",
    "- Integrate this DQ notebook in CI (smaller synthetic samples) to prevent regressions.\n",
    "- (Optional) Add Deequ checks into automated pipelines for continuous monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd6b3e2-31b0-40b2-ab46-78987f986c93",
   "metadata": {},
   "source": [
    "## 11) Save curated data (example)\n",
    "**Purpose:** Demonstrate how to persist the cleaned DataFrame into Parquet format with partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8fbc1-d122-4b63-be62-97f278a9ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "val OUTPUT_PATH = \"/Users/Felipe/lastfm/output/curated\"\n",
    "\n",
    "dqDf.write\n",
    "  .mode(\"overwrite\")\n",
    "  .partitionBy(\"user_id\")\n",
    "  .parquet(OUTPUT_PATH)\n",
    "\n",
    "println(s\"Curated dataset saved to ${OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5306f5c2-d6b2-4d42-92b9-f7a003a4cc5e",
   "metadata": {},
   "source": [
    "## 12) Export DQ metrics\n",
    "**Purpose:** Persist the summary metrics into a CSV/Parquet for reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92c1b2-2c60-4eeb-b8aa-06c2621a3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val dqMetrics = Seq(\n",
    "  (\"rows_read\", rowsRead.toString),\n",
    "  (\"rows_valid\", totalValid.toString),\n",
    "  (\"rows_dropped\", totalDropped.toString),\n",
    "  (\"missing_track_id\", missingTrackId.toString),\n",
    "  (\"pct_missing_track_id\", f\"${pctMissingTrackId}%.2f%%\")\n",
    ").toDF(\"metric\",\"value\")\n",
    "\n",
    "val METRICS_PATH = \"/Users/Felipe/lastfm/output/metrics\"\n",
    "\n",
    "dqMetrics.write.mode(\"overwrite\").option(\"header\",\"true\").csv(METRICS_PATH)\n",
    "\n",
    "println(s\"DQ metrics exported to ${METRICS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eef5407-65cb-415f-8505-c2bcaa01b96e",
   "metadata": {},
   "source": [
    "## 13) Join with profile data (optional)\n",
    "**Purpose:** Combine plays with user profile info for enriched analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326eb242-7f97-461b-a556-45e0858db32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val profileSchema = StructType(Seq(\n",
    "  StructField(\"user_id\", StringType, nullable = false),\n",
    "  StructField(\"gender\", StringType, nullable = true),\n",
    "  StructField(\"age\", IntegerType, nullable = true),\n",
    "  StructField(\"country\", StringType, nullable = true),\n",
    "  StructField(\"signup\", StringType, nullable = true)\n",
    "))\n",
    "\n",
    "val profileDf = spark.read\n",
    "  .option(\"sep\", \"\t\")\n",
    "  .option(\"header\", \"false\")\n",
    "  .schema(profileSchema)\n",
    "  .csv(PROFILE_PATH)\n",
    "\n",
    "val enrichedDf = dqDf.join(profileDf, Seq(\"user_id\"), \"left\")\n",
    "\n",
    "enrichedDf.show(SAMPLE_ROWS, truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb0df1-7125-49c8-a982-41714f898635",
   "metadata": {},
   "source": [
    "## 14) Wrap-up\n",
    "**Key takeaways:**\n",
    "- Data quality checks caught invalid timestamps, missing IDs, and empty fields.\n",
    "- Clear policies and documented assumptions make the pipeline reproducible.\n",
    "- Outputs (curated plays + metrics + profiles) are ready for downstream use in sessionization and top-track analysis.\n",
    "\n",
    "**End of Notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee78707b-7e87-4193-bcc1-4a7277aaa8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
