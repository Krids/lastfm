{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66755717-9890-40ff-a228-2dd91d5c6a0f",
   "metadata": {},
   "source": [
    "# Last.fm — Data Quality Notebook (Scala + Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea62fad-bd22-4dba-827b-7e8e2a4bda80",
   "metadata": {},
   "source": [
    "**Goal:** Manually verify (and document) data quality for the Last.fm 1K users dataset using **Scala + Spark**.\n",
    "\n",
    "**What this notebook covers:**\n",
    "1. Ingestion with **explicit schema** and **UTC timezone**.\n",
    "2. **Robust timestamp parsing** and counting invalid rows.\n",
    "3. **Key normalization**: prefer `track_id` (MBID), fallback to `artist_name — track_name`.\n",
    "4. **String sanitization** (trim, remove control chars).\n",
    "5. **Data quality metrics** (read/valid/dropped rows, % missing MBIDs).\n",
    "6. **Policy for empty fields** (user_id / artist_name / track_name).\n",
    "7. **Semantic rule checks** for session gaps: **= 20 min** vs **> 20 min**.\n",
    "8. **Duplicate detection** (same user, same timestamp, same track).\n",
    "9. *(Optional)* **Deequ** constraints (nullability, uniqueness).\n",
    "\n",
    "**Tested/compatible with:** Scala **2.12** and Spark **3.5.x**. Adjust library coordinates accordingly if you add optional Deequ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eda3bd9-9d9b-4580-8204-599edd2244eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.52.2.min',\n",
       "    jquery: 'https://code.jquery.com/jquery-3.3.1.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:10.726 [scala-interpreter-1] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using INPUT_PATH  = /Users/Felipe/lastfm/data/lastfm/lastfm-dataset-1k/userid-timestamp-artid-artname-traid-traname.tsv\n",
      "Using PROFILE_PATH = /Users/Felipe/lastfm/data/lastfm/lastfm-dataset-1k/userid-profile.tsv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._, plotly.element._, plotly.layout._, plotly.Almond._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{SparkSession, DataFrame}\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.logging.log4j.{LogManager, Level => LogLevel}\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.logging.log4j.core.Logger\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\u001b[39m\n",
       "\u001b[36mres1_9\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32mnull\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@1750c8c7\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m\n",
       "\u001b[36mINPUT_PATH\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/Users/Felipe/lastfm/data/lastfm/lastfm-dataset-1k/userid-timestamp-artid-artname-traid-traname.tsv\"\u001b[39m\n",
       "\u001b[36mPROFILE_PATH\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/Users/Felipe/lastfm/data/lastfm/lastfm-dataset-1k/userid-profile.tsv\"\u001b[39m\n",
       "\u001b[36mSAMPLE_ROWS\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m20\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.1`\n",
    "import $ivy.`org.plotly-scala::plotly-almond:0.8.0`\n",
    "import plotly._, plotly.element._, plotly.layout._, plotly.Almond._\n",
    "init()\n",
    "\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.logging.log4j.{LogManager, Level => LogLevel}\n",
    "import org.apache.logging.log4j.core.Logger\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// Suppress INFO logs\n",
    "System.setProperty(\"log4j2.level\", \"WARN\")\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"LastFM-DataCleaning\")\n",
    "  .master(\"local[*]\")\n",
    "  .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "  .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "// Reduz log para ERROR em loggers Spark e Hadoop\n",
    "Seq(\n",
    "  \"org.apache.spark\",\n",
    "  \"org.apache.spark.sql.execution\",\n",
    "  \"org.apache.spark.storage\",\n",
    "  \"org.apache.hadoop\",\n",
    "  \"org.spark_project\"\n",
    ").foreach { name =>\n",
    "  LogManager.getLogger(name).asInstanceOf[Logger].setLevel(LogLevel.ERROR)\n",
    "}\n",
    "\n",
    "LogManager.getRootLogger.asInstanceOf[Logger].setLevel(LogLevel.ERROR)\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "// Defaults\n",
    "var INPUT_PATH: String = \"/Users/Felipe/lastfm/data/lastfm/lastfm-dataset-1k/userid-timestamp-artid-artname-traid-traname.tsv\" // main play logs TSV\n",
    "var PROFILE_PATH: String = \"/Users/Felipe/lastfm/data/lastfm/lastfm-dataset-1k/userid-profile.tsv\"\n",
    "val SAMPLE_ROWS = 20 // number of rows to show in samples\n",
    "\n",
    "println(s\"Using INPUT_PATH  = ${INPUT_PATH}\")\n",
    "println(s\"Using PROFILE_PATH = ${PROFILE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad9d231-468b-46cd-b706-a4996b0c99d1",
   "metadata": {},
   "source": [
    "## 1) Ingestion with explicit schema\n",
    "**Purpose:** Avoid incorrect type inference and lock expected column order.\n",
    "\n",
    "**Columns:**\n",
    " - `user_id` (String, not null)\n",
    " - `ts_str` (String, not null) — raw timestamp to be parsed later\n",
    " - `artist_id` (String, nullable) — MBID\n",
    " - `artist_name` (String, nullable)\n",
    " - `track_id` (String, nullable) — MBID\n",
    " - `track_name` (String, nullable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3671a2a-3dbb-40e9-b577-d1f34362b24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows read (raw): 19150868\n",
      "+-----------+--------------------+------------------------------------+---------------+------------------------------------+------------------------------------------+\n",
      "|user_id    |ts_str              |artist_id                           |artist_name    |track_id                            |track_name                                |\n",
      "+-----------+--------------------+------------------------------------+---------------+------------------------------------+------------------------------------------+\n",
      "|user_000001|2009-05-04T23:08:57Z|f1b1cf71-bd35-4e99-8624-24a6e15f133a|Deep Dish      |NULL                                |Fuck Me Im Famous (Pacha Ibiza)-09-28-2007|\n",
      "|user_000001|2009-05-04T13:54:10Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Composition 0919 (Live_2009_4_15)         |\n",
      "|user_000001|2009-05-04T13:52:04Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Mc2 (Live_2009_4_15)                      |\n",
      "|user_000001|2009-05-04T13:42:52Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Hibari (Live_2009_4_15)                   |\n",
      "|user_000001|2009-05-04T13:42:11Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Mc1 (Live_2009_4_15)                      |\n",
      "|user_000001|2009-05-04T13:38:31Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |To Stanford (Live_2009_4_15)              |\n",
      "|user_000001|2009-05-04T13:33:28Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Improvisation (Live_2009_4_15)            |\n",
      "|user_000001|2009-05-04T13:23:45Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Glacier (Live_2009_4_15)                  |\n",
      "|user_000001|2009-05-04T13:19:22Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Parolibre (Live_2009_4_15)                |\n",
      "|user_000001|2009-05-04T13:13:38Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Bibo No Aozora (Live_2009_4_15)           |\n",
      "|user_000001|2009-05-04T13:06:09Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |f7c1f8f8-b935-45ed-8fc8-7def69d92a10|The Last Emperor (Theme)                  |\n",
      "|user_000001|2009-05-04T13:00:48Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Happyend (Live_2009_4_15)                 |\n",
      "|user_000001|2009-05-04T12:55:34Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |475d4e50-cebb-4cd0-8cd4-c3df97987962|Tibetan Dance (Version)                   |\n",
      "|user_000001|2009-05-04T12:51:26Z|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Behind The Mask (Live_2009_4_15)          |\n",
      "|user_000001|2009-05-03T15:48:25Z|ba2f4f3b-0293-4bc8-bb94-2f73b5207343|Underworld     |dc394163-2b78-4b56-94e4-658597a29ef8|Boy, Boy, Boy (Switch Remix)              |\n",
      "|user_000001|2009-05-03T15:37:56Z|ba2f4f3b-0293-4bc8-bb94-2f73b5207343|Underworld     |340d9a0b-9a43-4098-b116-9f79811bd508|Crocodile (Innervisions Orchestra Mix)    |\n",
      "|user_000001|2009-05-03T15:14:53Z|a16e47f5-aa54-47fe-87e4-bb8af91a9fdd|Ennio Morricone|0b04407b-f517-4e00-9e6a-494795efc73e|Ninna Nanna In Blu (Raw Deal Remix)       |\n",
      "|user_000001|2009-05-03T15:10:18Z|463a94f1-2713-40b1-9c88-dcc9c0170cae|Minus 8        |4e78efc4-e545-47af-9617-05ff816d86e2|Elysian Fields                            |\n",
      "|user_000001|2009-05-03T15:04:31Z|ad0811ea-e213-451d-b22f-fa1a7f9e0226|Beanfield      |fb51d2c4-cc69-4128-92f5-77ec38d66859|Planetary Deadlock                        |\n",
      "|user_000001|2009-05-03T14:56:25Z|309e2dfc-678e-4d09-a7a4-8eab9525b669|Dj Linus       |4277434f-e3c2-41ae-9ce3-23fd157f9347|Good Morning Love Coffee Is Ready         |\n",
      "+-----------+--------------------+------------------------------------+---------------+------------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- ts_str: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mschema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mSeq\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"user_id\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mfalse\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"ts_str\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mfalse\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"artist_id\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"artist_name\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"track_id\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"track_name\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  )\n",
       ")\n",
       "\u001b[36mrawDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_id: string, ts_str: string ... 4 more fields]\n",
       "\u001b[36mrowsRead\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m19150868L\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = StructType(Seq(\n",
    "  StructField(\"user_id\", StringType, nullable = false),\n",
    "  StructField(\"ts_str\", StringType, nullable = false),\n",
    "  StructField(\"artist_id\", StringType, nullable = true),\n",
    "  StructField(\"artist_name\", StringType, nullable = true),\n",
    "  StructField(\"track_id\", StringType, nullable = true),\n",
    "  StructField(\"track_name\", StringType, nullable = true)\n",
    "))\n",
    "\n",
    "val rawDf = spark.read\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .option(\"header\", \"false\")\n",
    "  .schema(schema)\n",
    "  .csv(INPUT_PATH)\n",
    "\n",
    "val rowsRead = rawDf.count()\n",
    "println(s\"Rows read (raw): ${rowsRead}\")\n",
    "rawDf.show(SAMPLE_ROWS, truncate = false)\n",
    "rawDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5366da0-54a9-4320-a368-8244409586e0",
   "metadata": {},
   "source": [
    "## 2) Robust timestamp parsing & invalid row accounting\n",
    "**Purpose:** Parse `ts_str` into a proper `timestamp` column (`ts`) and count invalid rows. Keep only rows with a valid timestamp.\n",
    "\n",
    "**Format used:** `yyyy-MM-dd'T'HH:mm:ss` (UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f45db9-b2ce-421a-a112-d40f25ad44fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid due to timestamp parse: 0\n",
      "Valid rows after ts parse: 19150868\n",
      "+-----------+------------------------------------+---------------+------------------------------------+------------------------------------------+-------------------+\n",
      "|user_id    |artist_id                           |artist_name    |track_id                            |track_name                                |ts                 |\n",
      "+-----------+------------------------------------+---------------+------------------------------------+------------------------------------------+-------------------+\n",
      "|user_000001|f1b1cf71-bd35-4e99-8624-24a6e15f133a|Deep Dish      |NULL                                |Fuck Me Im Famous (Pacha Ibiza)-09-28-2007|2009-05-04 23:08:57|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Composition 0919 (Live_2009_4_15)         |2009-05-04 13:54:10|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Mc2 (Live_2009_4_15)                      |2009-05-04 13:52:04|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Hibari (Live_2009_4_15)                   |2009-05-04 13:42:52|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Mc1 (Live_2009_4_15)                      |2009-05-04 13:42:11|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |To Stanford (Live_2009_4_15)              |2009-05-04 13:38:31|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Improvisation (Live_2009_4_15)            |2009-05-04 13:33:28|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Glacier (Live_2009_4_15)                  |2009-05-04 13:23:45|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Parolibre (Live_2009_4_15)                |2009-05-04 13:19:22|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Bibo No Aozora (Live_2009_4_15)           |2009-05-04 13:13:38|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |f7c1f8f8-b935-45ed-8fc8-7def69d92a10|The Last Emperor (Theme)                  |2009-05-04 13:06:09|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Happyend (Live_2009_4_15)                 |2009-05-04 13:00:48|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |475d4e50-cebb-4cd0-8cd4-c3df97987962|Tibetan Dance (Version)                   |2009-05-04 12:55:34|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Behind The Mask (Live_2009_4_15)          |2009-05-04 12:51:26|\n",
      "|user_000001|ba2f4f3b-0293-4bc8-bb94-2f73b5207343|Underworld     |dc394163-2b78-4b56-94e4-658597a29ef8|Boy, Boy, Boy (Switch Remix)              |2009-05-03 15:48:25|\n",
      "|user_000001|ba2f4f3b-0293-4bc8-bb94-2f73b5207343|Underworld     |340d9a0b-9a43-4098-b116-9f79811bd508|Crocodile (Innervisions Orchestra Mix)    |2009-05-03 15:37:56|\n",
      "|user_000001|a16e47f5-aa54-47fe-87e4-bb8af91a9fdd|Ennio Morricone|0b04407b-f517-4e00-9e6a-494795efc73e|Ninna Nanna In Blu (Raw Deal Remix)       |2009-05-03 15:14:53|\n",
      "|user_000001|463a94f1-2713-40b1-9c88-dcc9c0170cae|Minus 8        |4e78efc4-e545-47af-9617-05ff816d86e2|Elysian Fields                            |2009-05-03 15:10:18|\n",
      "|user_000001|ad0811ea-e213-451d-b22f-fa1a7f9e0226|Beanfield      |fb51d2c4-cc69-4128-92f5-77ec38d66859|Planetary Deadlock                        |2009-05-03 15:04:31|\n",
      "|user_000001|309e2dfc-678e-4d09-a7a4-8eab9525b669|Dj Linus       |4277434f-e3c2-41ae-9ce3-23fd157f9347|Good Morning Love Coffee Is Ready         |2009-05-03 14:56:25|\n",
      "+-----------+------------------------------------+---------------+------------------------------------+------------------------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mwithTsDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_id: string, artist_id: string ... 4 more fields]\n",
       "\u001b[36minvalidTsCount\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m0L\u001b[39m\n",
       "\u001b[36mvalidTsDf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [user_id: string, artist_id: string ... 4 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val withTsDf = rawDf\n",
    "  .withColumn(\"ts\", to_timestamp(col(\"ts_str\")))  // No format specified - uses ISO 8601 parsing\n",
    "  .drop(\"ts_str\")\n",
    "\n",
    "val invalidTsCount = withTsDf.filter(col(\"ts\").isNull).count()\n",
    "val validTsDf = withTsDf.filter(col(\"ts\").isNotNull)\n",
    "\n",
    "println(s\"Invalid due to timestamp parse: ${invalidTsCount}\")\n",
    "println(s\"Valid rows after ts parse: ${validTsDf.count()}\")\n",
    "\n",
    "validTsDf.show(SAMPLE_ROWS, truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07118f84-37f5-4ce9-8f3a-84968ca905c4",
   "metadata": {},
   "source": [
    "## 3) Key normalization (MBID preferred; fallback to `artist_name — track_name`)\n",
    "**Purpose:** Build a stable `track_key` used for counts/joins even when MBIDs are missing.\n",
    "\n",
    "**Rule:** If `track_id` (MBID) is present & non-empty, use it; otherwise use `${artist_name} — ${track_name}` with nulls replaced by `?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b331dc4-91f5-425a-bffd-f27f684f3ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+------------------------------------+---------------+------------------------------------+------------------------------------------+------------------------------------------------------+\n",
      "|user_id    |ts                 |artist_id                           |artist_name    |track_id                            |track_name                                |track_key                                             |\n",
      "+-----------+-------------------+------------------------------------+---------------+------------------------------------+------------------------------------------+------------------------------------------------------+\n",
      "|user_000001|2009-05-04 23:08:57|f1b1cf71-bd35-4e99-8624-24a6e15f133a|Deep Dish      |NULL                                |Fuck Me Im Famous (Pacha Ibiza)-09-28-2007|Deep Dish — Fuck Me Im Famous (Pacha Ibiza)-09-28-2007|\n",
      "|user_000001|2009-05-04 13:54:10|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Composition 0919 (Live_2009_4_15)         |坂本龍一 — Composition 0919 (Live_2009_4_15)          |\n",
      "|user_000001|2009-05-04 13:52:04|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Mc2 (Live_2009_4_15)                      |坂本龍一 — Mc2 (Live_2009_4_15)                       |\n",
      "|user_000001|2009-05-04 13:42:52|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Hibari (Live_2009_4_15)                   |坂本龍一 — Hibari (Live_2009_4_15)                    |\n",
      "|user_000001|2009-05-04 13:42:11|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Mc1 (Live_2009_4_15)                      |坂本龍一 — Mc1 (Live_2009_4_15)                       |\n",
      "|user_000001|2009-05-04 13:38:31|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |To Stanford (Live_2009_4_15)              |坂本龍一 — To Stanford (Live_2009_4_15)               |\n",
      "|user_000001|2009-05-04 13:33:28|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Improvisation (Live_2009_4_15)            |坂本龍一 — Improvisation (Live_2009_4_15)             |\n",
      "|user_000001|2009-05-04 13:23:45|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Glacier (Live_2009_4_15)                  |坂本龍一 — Glacier (Live_2009_4_15)                   |\n",
      "|user_000001|2009-05-04 13:19:22|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Parolibre (Live_2009_4_15)                |坂本龍一 — Parolibre (Live_2009_4_15)                 |\n",
      "|user_000001|2009-05-04 13:13:38|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Bibo No Aozora (Live_2009_4_15)           |坂本龍一 — Bibo No Aozora (Live_2009_4_15)            |\n",
      "|user_000001|2009-05-04 13:06:09|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |f7c1f8f8-b935-45ed-8fc8-7def69d92a10|The Last Emperor (Theme)                  |f7c1f8f8-b935-45ed-8fc8-7def69d92a10                  |\n",
      "|user_000001|2009-05-04 13:00:48|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Happyend (Live_2009_4_15)                 |坂本龍一 — Happyend (Live_2009_4_15)                  |\n",
      "|user_000001|2009-05-04 12:55:34|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |475d4e50-cebb-4cd0-8cd4-c3df97987962|Tibetan Dance (Version)                   |475d4e50-cebb-4cd0-8cd4-c3df97987962                  |\n",
      "|user_000001|2009-05-04 12:51:26|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Behind The Mask (Live_2009_4_15)          |坂本龍一 — Behind The Mask (Live_2009_4_15)           |\n",
      "|user_000001|2009-05-03 15:48:25|ba2f4f3b-0293-4bc8-bb94-2f73b5207343|Underworld     |dc394163-2b78-4b56-94e4-658597a29ef8|Boy, Boy, Boy (Switch Remix)              |dc394163-2b78-4b56-94e4-658597a29ef8                  |\n",
      "|user_000001|2009-05-03 15:37:56|ba2f4f3b-0293-4bc8-bb94-2f73b5207343|Underworld     |340d9a0b-9a43-4098-b116-9f79811bd508|Crocodile (Innervisions Orchestra Mix)    |340d9a0b-9a43-4098-b116-9f79811bd508                  |\n",
      "|user_000001|2009-05-03 15:14:53|a16e47f5-aa54-47fe-87e4-bb8af91a9fdd|Ennio Morricone|0b04407b-f517-4e00-9e6a-494795efc73e|Ninna Nanna In Blu (Raw Deal Remix)       |0b04407b-f517-4e00-9e6a-494795efc73e                  |\n",
      "|user_000001|2009-05-03 15:10:18|463a94f1-2713-40b1-9c88-dcc9c0170cae|Minus 8        |4e78efc4-e545-47af-9617-05ff816d86e2|Elysian Fields                            |4e78efc4-e545-47af-9617-05ff816d86e2                  |\n",
      "|user_000001|2009-05-03 15:04:31|ad0811ea-e213-451d-b22f-fa1a7f9e0226|Beanfield      |fb51d2c4-cc69-4128-92f5-77ec38d66859|Planetary Deadlock                        |fb51d2c4-cc69-4128-92f5-77ec38d66859                  |\n",
      "|user_000001|2009-05-03 14:56:25|309e2dfc-678e-4d09-a7a4-8eab9525b669|Dj Linus       |4277434f-e3c2-41ae-9ce3-23fd157f9347|Good Morning Love Coffee Is Ready         |4277434f-e3c2-41ae-9ce3-23fd157f9347                  |\n",
      "+-----------+-------------------+------------------------------------+---------------+------------------------------------+------------------------------------------+------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mnormalizedDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_id: string, artist_id: string ... 5 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val normalizedDf = validTsDf.withColumn(\n",
    "  \"track_key\",\n",
    "  when(col(\"track_id\").isNotNull && length(col(\"track_id\")) > 0, col(\"track_id\"))\n",
    "    .otherwise(concat_ws(\" — \", coalesce(col(\"artist_name\"), lit(\"?\")), coalesce(col(\"track_name\"), lit(\"?\"))))\n",
    ")\n",
    "\n",
    "normalizedDf.select(\"user_id\",\"ts\",\"artist_id\",\"artist_name\",\"track_id\",\"track_name\",\"track_key\")\n",
    "  .show(SAMPLE_ROWS, truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c7a759-d83a-4eb6-b97f-a6f9b62a89d9",
   "metadata": {},
   "source": [
    "## 4) String sanitization\n",
    "**Purpose:** Remove control characters and trim whitespace from key string fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae741807-93a3-4ce7-b321-1246019f7bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+------------------------------------------+\n",
      "|user_id    |artist_name    |track_name                                |\n",
      "+-----------+---------------+------------------------------------------+\n",
      "|user_000001|Deep Dish      |Fuck Me Im Famous (Pacha Ibiza)-09-28-2007|\n",
      "|user_000001|坂本龍一       |Composition 0919 (Live_2009_4_15)         |\n",
      "|user_000001|坂本龍一       |Mc2 (Live_2009_4_15)                      |\n",
      "|user_000001|坂本龍一       |Hibari (Live_2009_4_15)                   |\n",
      "|user_000001|坂本龍一       |Mc1 (Live_2009_4_15)                      |\n",
      "|user_000001|坂本龍一       |To Stanford (Live_2009_4_15)              |\n",
      "|user_000001|坂本龍一       |Improvisation (Live_2009_4_15)            |\n",
      "|user_000001|坂本龍一       |Glacier (Live_2009_4_15)                  |\n",
      "|user_000001|坂本龍一       |Parolibre (Live_2009_4_15)                |\n",
      "|user_000001|坂本龍一       |Bibo No Aozora (Live_2009_4_15)           |\n",
      "|user_000001|坂本龍一       |The Last Emperor (Theme)                  |\n",
      "|user_000001|坂本龍一       |Happyend (Live_2009_4_15)                 |\n",
      "|user_000001|坂本龍一       |Tibetan Dance (Version)                   |\n",
      "|user_000001|坂本龍一       |Behind The Mask (Live_2009_4_15)          |\n",
      "|user_000001|Underworld     |Boy, Boy, Boy (Switch Remix)              |\n",
      "|user_000001|Underworld     |Crocodile (Innervisions Orchestra Mix)    |\n",
      "|user_000001|Ennio Morricone|Ninna Nanna In Blu (Raw Deal Remix)       |\n",
      "|user_000001|Minus 8        |Elysian Fields                            |\n",
      "|user_000001|Beanfield      |Planetary Deadlock                        |\n",
      "|user_000001|Dj Linus       |Good Morning Love Coffee Is Ready         |\n",
      "+-----------+---------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msanitizeUdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mSparkUserDefinedFunction\u001b[39m(\n",
       "  f = ammonite.$sess.cmd5$Helper$$Lambda$7274/0x0000000a0205b040@36a354f6,\n",
       "  dataType = StringType,\n",
       "  inputEncoders = \u001b[33mList\u001b[39m(\n",
       "    \u001b[33mSome\u001b[39m(\n",
       "      value = \u001b[33mExpressionEncoder\u001b[39m(\n",
       "        objSerializer = \u001b[33mStaticInvoke\u001b[39m(\n",
       "          staticObject = class org.apache.spark.unsafe.types.UTF8String,\n",
       "          dataType = StringType,\n",
       "          functionName = \u001b[32m\"fromString\"\u001b[39m,\n",
       "          arguments = \u001b[33mList\u001b[39m(\n",
       "            \u001b[33mBoundReference\u001b[39m(\n",
       "              ordinal = \u001b[32m0\u001b[39m,\n",
       "              dataType = \u001b[33mObjectType\u001b[39m(cls = class java.lang.String),\n",
       "              nullable = \u001b[32mtrue\u001b[39m\n",
       "            )\n",
       "          ),\n",
       "          inputTypes = \u001b[33mList\u001b[39m(),\n",
       "          propagateNull = \u001b[32mtrue\u001b[39m,\n",
       "          returnNullable = \u001b[32mfalse\u001b[39m,\n",
       "          isDeterministic = \u001b[32mtrue\u001b[39m\n",
       "        ),\n",
       "        objDeserializer = \u001b[33mInvoke\u001b[39m(\n",
       "          targetObject = \u001b[33mUpCast\u001b[39m(\n",
       "            child = \u001b[33mGetColumnByOrdinal\u001b[39m(ordinal = \u001b[32m0\u001b[39m, dataType = StringType),\n",
       "            target = StringType,\n",
       "            walkedTypePath = \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"java.lang.String\\\"\"\u001b[39m)\n",
       "          ),\n",
       "          functionName = \u001b[32m\"toString\"\u001b[39m,\n",
       "          dataType = \u001b[33mObjectType\u001b[39m(cls = class java.lang.String),\n",
       "          arguments = \u001b[33mList\u001b[39m(),\n",
       "          methodInputTypes = \u001b[33mList\u001b[39m(),\n",
       "          propagateNull = \u001b[32mtrue\u001b[39m,\n",
       "          returnNullable = \u001b[32mfalse\u001b[39m,\n",
       "          isDeterministic = \u001b[32mtrue\u001b[39m\n",
       "        ),\n",
       "        clsTag = java.lang.String\n",
       "      )\n",
       "    )\n",
       "...\n",
       "\u001b[36mcleanDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_id: string, artist_id: string ... 5 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sanitizeUdf = udf { s: String =>\n",
    "  if (s == null) null\n",
    "  else s.replaceAll(\"\\\\p{Cntrl}\", \"\").trim\n",
    "}\n",
    "\n",
    "val cleanDf = normalizedDf\n",
    "  .withColumn(\"artist_name\", sanitizeUdf(col(\"artist_name\")))\n",
    "  .withColumn(\"track_name\", sanitizeUdf(col(\"track_name\")))\n",
    "  .withColumn(\"user_id\", sanitizeUdf(col(\"user_id\")))\n",
    "\n",
    "cleanDf.select(\"user_id\",\"artist_name\",\"track_name\").show(SAMPLE_ROWS, truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29afadfd-aa24-4c65-bbcc-7a81c4c019fc",
   "metadata": {},
   "source": [
    "## 5) Data Quality metrics summary\n",
    "**Purpose:** Summarize read/valid/dropped counts and percent of missing MBIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1765d8-4b74-4105-a942-53de89b082f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcalculateDataQualityMetrics\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mDataQualityReport\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mgenerateQualityReport\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculateDataQualityMetrics(originalCount: Long, finalDf: DataFrame): Map[String, Any] = {\n",
    "  val finalCount = finalDf.count()\n",
    "  val missingTrackId = finalDf.filter(col(\"track_id\").isNull || length(col(\"track_id\")) === 0).count()\n",
    "  \n",
    "  Map(\n",
    "    \"rows_read\" -> originalCount,\n",
    "    \"rows_valid\" -> finalCount,\n",
    "    \"rows_dropped\" -> (originalCount - finalCount),\n",
    "    \"missing_track_id\" -> missingTrackId,\n",
    "    \"pct_missing_track_id\" -> (if (finalCount == 0) 0.0 else missingTrackId.toDouble / finalCount * 100.0)\n",
    "  )\n",
    "}\n",
    "\n",
    "case class DataQualityReport(\n",
    "  timestamp: String,\n",
    "  dataset: String,\n",
    "  metrics: Map[String, Any],\n",
    "  warnings: List[String] = List.empty,\n",
    "  recommendations: List[String] = List.empty\n",
    ")\n",
    "\n",
    "def generateQualityReport(originalCount: Long, cleanDf: DataFrame): DataQualityReport = {\n",
    "  val metrics = calculateDataQualityMetrics(originalCount, cleanDf)\n",
    "  val warnings = scala.collection.mutable.ListBuffer[String]()\n",
    "  val recommendations = scala.collection.mutable.ListBuffer[String]()\n",
    "  \n",
    "  // Add business logic checks\n",
    "  val pctMissing = metrics(\"pct_missing_track_id\").asInstanceOf[Double]\n",
    "  if (pctMissing > 50.0) {\n",
    "    warnings += s\"High percentage of missing track IDs: ${pctMissing}%\"\n",
    "    recommendations += \"Consider improving track ID matching or using fallback keys\"\n",
    "  }\n",
    "  \n",
    "  DataQualityReport(\n",
    "    timestamp = java.time.Instant.now().toString,\n",
    "    dataset = \"lastfm-1k\",\n",
    "    metrics = metrics,\n",
    "    warnings = warnings.toList,\n",
    "    recommendations = recommendations.toList\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06d52cec-eebe-44cd-b863-d86615acc308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING DATA QUALITY REPORT\n",
      "============================================================\n",
      "\n",
      "📊 Data Quality Report (lastfm-1k)\n",
      "🕐 Generated: 2025-09-11T06:00:29.963686Z\n",
      "\n",
      "📈 METRICS:\n",
      "   Rows valid               : 19150868\n",
      "   Rows read                : 19150868\n",
      "   Missing track id         : 2168588\n",
      "   Pct missing track id     : 11.323706058649666\n",
      "   Rows dropped             : 0\n",
      "\n",
      "✅ Data quality assessment completed successfully!\n"
     ]
    }
   ],
   "source": [
    "// Place this after your data cleaning pipeline, before the final summary\n",
    "println(\"\\n\" + \"=\" * 60)\n",
    "println(\"GENERATING DATA QUALITY REPORT\")\n",
    "println(\"=\" * 60)\n",
    "\n",
    "try {\n",
    "  val qualityReport = generateQualityReport(rowsRead, cleanDf)\n",
    "  \n",
    "  // Display report\n",
    "  println(s\"\\n📊 Data Quality Report (${qualityReport.dataset})\")\n",
    "  println(s\"🕐 Generated: ${qualityReport.timestamp}\")\n",
    "  \n",
    "  println(s\"\\n📈 METRICS:\")\n",
    "  qualityReport.metrics.foreach { case (key, value) =>\n",
    "    val formattedKey = key.replace(\"_\", \" \").capitalize\n",
    "    println(f\"   $formattedKey%-25s: $value\")\n",
    "  }\n",
    "  \n",
    "  if (qualityReport.warnings.nonEmpty) {\n",
    "    println(s\"\\n⚠️  WARNINGS (${qualityReport.warnings.length}):\")\n",
    "    qualityReport.warnings.zipWithIndex.foreach { case (warning, idx) =>\n",
    "      println(s\"   ${idx + 1}. $warning\")\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  if (qualityReport.recommendations.nonEmpty) {\n",
    "    println(s\"\\n💡 RECOMMENDATIONS (${qualityReport.recommendations.length}):\")\n",
    "    qualityReport.recommendations.zipWithIndex.foreach { case (rec, idx) =>\n",
    "      println(s\"   ${idx + 1}. $rec\")\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  println(s\"\\n✅ Data quality assessment completed successfully!\")\n",
    "  \n",
    "} catch {\n",
    "  case ex: Exception =>\n",
    "    println(s\"❌ Error generating quality report: ${ex.getMessage}\")\n",
    "    ex.printStackTrace()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70f8bae6-76e8-429f-a522-c8d13c6f9697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mvalidateDataQuality\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validateDataQuality(df: DataFrame): List[String] = {\n",
    "  val issues = scala.collection.mutable.ListBuffer[String]()\n",
    "  \n",
    "  // Check for negative timestamps (if converted to epoch)\n",
    "  val futureTimestamps = df.filter(col(\"ts\") > current_timestamp()).count()\n",
    "  if (futureTimestamps > 0) {\n",
    "    issues += s\"Found $futureTimestamps records with future timestamps\"\n",
    "  }\n",
    "  \n",
    "  // Check for extremely old timestamps (before music streaming era)\n",
    "  val veryOldTimestamps = df.filter(col(\"ts\") < lit(\"2000-01-01\")).count()\n",
    "  if (veryOldTimestamps > 0) {\n",
    "    issues += s\"Found $veryOldTimestamps records with timestamps before 2000\"\n",
    "  }\n",
    "  \n",
    "  // Check for users with unrealistic play counts\n",
    "  val suspiciousUsers = df.groupBy(\"user_id\")\n",
    "    .count()\n",
    "    .filter(col(\"count\") > 100000) // Adjust threshold as needed\n",
    "    .count()\n",
    "  \n",
    "  if (suspiciousUsers > 0) {\n",
    "    issues += s\"Found $suspiciousUsers users with >100k plays (potential data quality issue)\"\n",
    "  }\n",
    "  \n",
    "  issues.toList\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4d3ac34-e557-4931-ad90-0d39e23d6330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List(Found 13 users with >100k plays (potential data quality issue))\n"
     ]
    }
   ],
   "source": [
    "println(validateDataQuality(cleanDf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019806a3-e77b-48dc-8550-3189058655c2",
   "metadata": {},
   "source": [
    "## 6) Empty-field policies\n",
    "**Purpose:** Inspect how many rows have empty `user_id`, `artist_name`, or `track_name` and decide whether to drop or impute.\n",
    "\n",
    "**Recommendation:** Drop rows with empty `user_id` and either drop or mark unknown `artist/track` depending on downstream needs (document in README).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e291dc-d97b-44b0-bba8-2606a65c5b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with empty user_id     : 0\n",
      "Rows with empty artist_name : 0\n",
      "Rows with empty track_name  : 8\n",
      "Rows after empty-field policy: 19150860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36memptyUser\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m0L\u001b[39m\n",
       "\u001b[36memptyArtist\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m0L\u001b[39m\n",
       "\u001b[36memptyTrack\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m8L\u001b[39m\n",
       "\u001b[36mDROP_EMPTY\u001b[39m: \u001b[32mBoolean\u001b[39m = \u001b[32mtrue\u001b[39m\n",
       "\u001b[36mdqDf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [user_id: string, artist_id: string ... 5 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val emptyUser = cleanDf.filter(coalesce(col(\"user_id\"), lit(\"\")) === \"\").count()\n",
    "val emptyArtist = cleanDf.filter(coalesce(col(\"artist_name\"), lit(\"\")) === \"\").count()\n",
    "val emptyTrack = cleanDf.filter(coalesce(col(\"track_name\"), lit(\"\")) === \"\").count()\n",
    "\n",
    "println(s\"Rows with empty user_id     : $emptyUser\")\n",
    "println(s\"Rows with empty artist_name : $emptyArtist\")\n",
    "println(s\"Rows with empty track_name  : $emptyTrack\")\n",
    "\n",
    "val DROP_EMPTY = true // toggle this policy if needed\n",
    "val dqDf = if (DROP_EMPTY) {\n",
    "  cleanDf.filter(col(\"user_id\") =!= \"\" && col(\"artist_name\") =!= \"\" && col(\"track_name\") =!= \"\")\n",
    "} else cleanDf\n",
    "\n",
    "println(s\"Rows after empty-field policy: ${dqDf.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a109a372-684a-40f3-8e20-a65279aebf36",
   "metadata": {},
   "source": [
    "## 7) Semantic rule — session gap boundary (=20 vs >20 minutes)\n",
    "**Purpose:** Build a tiny synthetic dataset to verify the session split rule at 20 minutes.\n",
    "\n",
    "**Rule:**\n",
    "- gap **≤ 20** minutes → same session\n",
    "- gap **> 20** minutes → new session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8c8a466-46d7-4ed3-96d4-89b8e7a5cd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+\n",
      "|user_id|ts                 |track_key|\n",
      "+-------+-------------------+---------+\n",
      "|u1     |2023-01-01 06:00:00|A        |\n",
      "|u1     |2023-01-01 06:20:00|B        |\n",
      "|u1     |2023-01-01 06:40:01|C        |\n",
      "|u2     |2023-01-01 05:00:00|X        |\n",
      "|u2     |2023-01-01 05:15:00|Y        |\n",
      "+-------+-------------------+---------+\n",
      "\n",
      "+-------+-------------------+---------+-------------------+------------------+--------------+-----------+\n",
      "|user_id|ts                 |track_key|prev_ts            |gap_minutes       |is_new_session|session_seq|\n",
      "+-------+-------------------+---------+-------------------+------------------+--------------+-----------+\n",
      "|u1     |2023-01-01 06:00:00|A        |NULL               |NULL              |1             |1          |\n",
      "|u1     |2023-01-01 06:20:00|B        |2023-01-01 06:00:00|20.0              |0             |1          |\n",
      "|u1     |2023-01-01 06:40:01|C        |2023-01-01 06:20:00|20.016666666666666|1             |2          |\n",
      "|u2     |2023-01-01 05:00:00|X        |NULL               |NULL              |1             |1          |\n",
      "|u2     |2023-01-01 05:15:00|Y        |2023-01-01 05:00:00|15.0              |0             |1          |\n",
      "+-------+-------------------+---------+-------------------+------------------+--------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mjava.sql.Timestamp\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mts\u001b[39m\n",
       "\u001b[36msessionCheck\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_id: string, ts: timestamp ... 1 more field]\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\u001b[39m\n",
       "\u001b[36mw\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@34064b08\n",
       "\u001b[36mprevTs\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mColumn\u001b[39m = lag(ts, 1, NULL) OVER (PARTITION BY user_id ORDER BY ts ASC NULLS FIRST unspecifiedframe$())\n",
       "\u001b[36mgapSec\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mColumn\u001b[39m = (CAST(ts AS BIGINT) - CAST(lag(ts, 1, NULL) OVER (PARTITION BY user_id ORDER BY ts ASC NULLS FIRST unspecifiedframe$()) AS BIGINT))\n",
       "\u001b[36mgapMin\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mColumn\u001b[39m = CASE WHEN (lag(ts, 1, NULL) OVER (PARTITION BY user_id ORDER BY ts ASC NULLS FIRST unspecifiedframe$()) IS NULL) THEN CAST(NULL AS DOUBLE) ELSE ((CAST(ts AS BIGINT) - CAST(lag(ts, 1, NULL) OVER (PARTITION BY user_id ORDER BY ts ASC NULLS FIRST unspecifiedframe$()) AS BIGINT)) / 60.0) END\n",
       "\u001b[36mwithGaps\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_id: string, ts: timestamp ... 5 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.sql.Timestamp\n",
    "\n",
    "def ts(s: String) = Timestamp.valueOf(s.replace(\"T\", \" \"))\n",
    "\n",
    "val sessionCheck = Seq(\n",
    "  (\"u1\", ts(\"2023-01-01T10:00:00\"), \"A\"),\n",
    "  (\"u1\", ts(\"2023-01-01T10:20:00\"), \"B\"), // exactly 20 min → SAME session\n",
    "  (\"u1\", ts(\"2023-01-01T10:40:01\"), \"C\"), // > 20 min → NEW session\n",
    "  (\"u2\", ts(\"2023-01-01T09:00:00\"), \"X\"),\n",
    "  (\"u2\", ts(\"2023-01-01T09:15:00\"), \"Y\")  // < 20 min → SAME session\n",
    ").toDF(\"user_id\",\"ts\",\"track_key\")\n",
    "\n",
    "sessionCheck.orderBy(\"user_id\",\"ts\").show(truncate = false)\n",
    "\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "val w = Window.partitionBy(\"user_id\").orderBy(col(\"ts\").asc)\n",
    "\n",
    "val prevTs = lag(col(\"ts\"), 1).over(w)\n",
    "val gapSec = (col(\"ts\").cast(\"long\") - prevTs.cast(\"long\"))\n",
    "val gapMin = when(prevTs.isNull, lit(null).cast(\"double\")).otherwise(gapSec / 60.0)\n",
    "\n",
    "val withGaps = sessionCheck\n",
    "  .withColumn(\"prev_ts\", prevTs)\n",
    "  .withColumn(\"gap_minutes\", gapMin)\n",
    "  .withColumn(\"is_new_session\", when(prevTs.isNull, 1).when(col(\"gap_minutes\") > 20.0, 1).otherwise(0))\n",
    "  .withColumn(\"session_seq\", sum(col(\"is_new_session\")).over(w))\n",
    "\n",
    "withGaps.orderBy(\"user_id\",\"ts\").show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f54b8dd-1b73-4569-97e8-b49b2a840ea8",
   "metadata": {},
   "source": [
    "## 8) Duplicate detection\n",
    "**Purpose:** Identify potential duplicates defined as **same user, same timestamp, same track**.\n",
    "\n",
    "**Action:** Count duplicates and preview a few; decide whether to drop or keep (document in README)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6f91527-90e1-447a-bd5b-28d6946acd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct duplicate keys: 1\n",
      "Total duplicated rows   : 2\n",
      "+-----------+-------------------+------------------------------------+---+\n",
      "|user_id    |ts                 |track_key                           |cnt|\n",
      "+-----------+-------------------+------------------------------------+---+\n",
      "|user_000274|2008-10-31 21:52:29|f8317357-7fea-47d4-9e2f-3fe91ef349c9|2  |\n",
      "+-----------+-------------------+------------------------------------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdupCols\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m\"user_id\"\u001b[39m, \u001b[32m\"ts\"\u001b[39m, \u001b[32m\"track_key\"\u001b[39m)\n",
       "\u001b[36mdupCounts\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [user_id: string, ts: timestamp ... 2 more fields]\n",
       "\u001b[36mtotalDupRows\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m2L\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dupCols = Seq(\"user_id\",\"ts\",\"track_key\")\n",
    "\n",
    "val dupCounts = dqDf\n",
    "  .groupBy(dupCols.map(col): _*)\n",
    "  .agg(count(lit(1)).alias(\"cnt\"))\n",
    "  .filter(col(\"cnt\") > 1)\n",
    "\n",
    "val totalDupRows = if (dupCounts.head(1).isEmpty) 0L else dupCounts.select(sum(\"cnt\")).first().getLong(0)\n",
    "\n",
    "println(s\"Distinct duplicate keys: ${dupCounts.count()}\")\n",
    "println(s\"Total duplicated rows   : ${totalDupRows}\")\n",
    "\n",
    "dupCounts.orderBy(col(\"cnt\").desc).show(20, truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e42caf2-e639-4207-8c38-e53b74e7afe9",
   "metadata": {},
   "source": [
    "## 9) (Optional) Deequ constraints\n",
    "**Purpose:** Validate constraints like nullability and uniqueness using **AWS Deequ**. This section is optional and requires adding Deequ as a dependency.\n",
    "\n",
    "**How to enable:**\n",
    "- Add library: `\"com.amazon.deequ\" %% \"deequ\" % \"2.0.7-spark-3.3\"` (or a version compatible with your Spark).\n",
    "- Then run checks such as: not-null on `user_id`, timestamp; uniqueness on `(user_id, ts)` if required.\n",
    "\n",
    "**Note:** The exact coordinates vary by Spark/Scala versions; confirm compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d449233-54a2-4e88-906d-f3c2ee1cd506",
   "metadata": {},
   "source": [
    "## 10) Summary & Next steps\n",
    "**What we verified:**\n",
    "- Explicit schema & UTC timezone.\n",
    "- Timestamp parsing with invalid-row accounting.\n",
    "- Track key normalization (MBID preferred, fallback safe).\n",
    "- String sanitization.\n",
    "- DQ metrics (read/valid/dropped, % missing MBIDs).\n",
    "- Empty-field policies and their impact.\n",
    "- Semantic rule at the 20-minute boundary (session split).\n",
    "- Duplicate detection and preview.\n",
    " \n",
    "**Next steps (suggested):**\n",
    "- Decide and enforce final policies (drop vs. impute) and document in README.\n",
    "- Persist cleaned datasets to a curated zone (e.g., Parquet, partitioned).\n",
    "- Integrate this DQ notebook in CI (smaller synthetic samples) to prevent regressions.\n",
    "- (Optional) Add Deequ checks into automated pipelines for continuous monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd6b3e2-31b0-40b2-ab46-78987f986c93",
   "metadata": {},
   "source": [
    "## 11) Save curated data (example)\n",
    "**Purpose:** Demonstrate how to persist the cleaned DataFrame into Parquet format with partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8fbc1-d122-4b63-be62-97f278a9ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "val OUTPUT_PATH = \"/Users/Felipe/lastfm/output/curated\"\n",
    "\n",
    "dqDf.write\n",
    "  .mode(\"overwrite\")\n",
    "  .partitionBy(\"user_id\")\n",
    "  .parquet(OUTPUT_PATH)\n",
    "\n",
    "println(s\"Curated dataset saved to ${OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5306f5c2-d6b2-4d42-92b9-f7a003a4cc5e",
   "metadata": {},
   "source": [
    "## 12) Export DQ metrics\n",
    "**Purpose:** Persist the summary metrics into a CSV/Parquet for reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92c1b2-2c60-4eeb-b8aa-06c2621a3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val dqMetrics = Seq(\n",
    "  (\"rows_read\", rowsRead.toString),\n",
    "  (\"rows_valid\", totalValid.toString),\n",
    "  (\"rows_dropped\", totalDropped.toString),\n",
    "  (\"missing_track_id\", missingTrackId.toString),\n",
    "  (\"pct_missing_track_id\", f\"${pctMissingTrackId}%.2f%%\")\n",
    ").toDF(\"metric\",\"value\")\n",
    "\n",
    "val METRICS_PATH = \"/Users/Felipe/lastfm/output/metrics\"\n",
    "\n",
    "dqMetrics.write.mode(\"overwrite\").option(\"header\",\"true\").csv(METRICS_PATH)\n",
    "\n",
    "println(s\"DQ metrics exported to ${METRICS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eef5407-65cb-415f-8505-c2bcaa01b96e",
   "metadata": {},
   "source": [
    "## 13) Join with profile data (optional)\n",
    "**Purpose:** Combine plays with user profile info for enriched analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "326eb242-7f97-461b-a556-45e0858db32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------------------+---------------+------------------------------------+------------------------------------------+-------------------+------------------------------------------------------+------+----+-------+------------+\n",
      "|user_id    |artist_id                           |artist_name    |track_id                            |track_name                                |ts                 |track_key                                             |gender|age |country|signup      |\n",
      "+-----------+------------------------------------+---------------+------------------------------------+------------------------------------------+-------------------+------------------------------------------------------+------+----+-------+------------+\n",
      "|user_000001|f1b1cf71-bd35-4e99-8624-24a6e15f133a|Deep Dish      |NULL                                |Fuck Me Im Famous (Pacha Ibiza)-09-28-2007|2009-05-04 23:08:57|Deep Dish — Fuck Me Im Famous (Pacha Ibiza)-09-28-2007|m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Composition 0919 (Live_2009_4_15)         |2009-05-04 13:54:10|坂本龍一 — Composition 0919 (Live_2009_4_15)          |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Mc2 (Live_2009_4_15)                      |2009-05-04 13:52:04|坂本龍一 — Mc2 (Live_2009_4_15)                       |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Hibari (Live_2009_4_15)                   |2009-05-04 13:42:52|坂本龍一 — Hibari (Live_2009_4_15)                    |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Mc1 (Live_2009_4_15)                      |2009-05-04 13:42:11|坂本龍一 — Mc1 (Live_2009_4_15)                       |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |To Stanford (Live_2009_4_15)              |2009-05-04 13:38:31|坂本龍一 — To Stanford (Live_2009_4_15)               |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Improvisation (Live_2009_4_15)            |2009-05-04 13:33:28|坂本龍一 — Improvisation (Live_2009_4_15)             |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Glacier (Live_2009_4_15)                  |2009-05-04 13:23:45|坂本龍一 — Glacier (Live_2009_4_15)                   |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Parolibre (Live_2009_4_15)                |2009-05-04 13:19:22|坂本龍一 — Parolibre (Live_2009_4_15)                 |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Bibo No Aozora (Live_2009_4_15)           |2009-05-04 13:13:38|坂本龍一 — Bibo No Aozora (Live_2009_4_15)            |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |f7c1f8f8-b935-45ed-8fc8-7def69d92a10|The Last Emperor (Theme)                  |2009-05-04 13:06:09|f7c1f8f8-b935-45ed-8fc8-7def69d92a10                  |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Happyend (Live_2009_4_15)                 |2009-05-04 13:00:48|坂本龍一 — Happyend (Live_2009_4_15)                  |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |475d4e50-cebb-4cd0-8cd4-c3df97987962|Tibetan Dance (Version)                   |2009-05-04 12:55:34|475d4e50-cebb-4cd0-8cd4-c3df97987962                  |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|a7f7df4a-77d8-4f12-8acd-5c60c93f4de8|坂本龍一       |NULL                                |Behind The Mask (Live_2009_4_15)          |2009-05-04 12:51:26|坂本龍一 — Behind The Mask (Live_2009_4_15)           |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|ba2f4f3b-0293-4bc8-bb94-2f73b5207343|Underworld     |dc394163-2b78-4b56-94e4-658597a29ef8|Boy, Boy, Boy (Switch Remix)              |2009-05-03 15:48:25|dc394163-2b78-4b56-94e4-658597a29ef8                  |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|ba2f4f3b-0293-4bc8-bb94-2f73b5207343|Underworld     |340d9a0b-9a43-4098-b116-9f79811bd508|Crocodile (Innervisions Orchestra Mix)    |2009-05-03 15:37:56|340d9a0b-9a43-4098-b116-9f79811bd508                  |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|a16e47f5-aa54-47fe-87e4-bb8af91a9fdd|Ennio Morricone|0b04407b-f517-4e00-9e6a-494795efc73e|Ninna Nanna In Blu (Raw Deal Remix)       |2009-05-03 15:14:53|0b04407b-f517-4e00-9e6a-494795efc73e                  |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|463a94f1-2713-40b1-9c88-dcc9c0170cae|Minus 8        |4e78efc4-e545-47af-9617-05ff816d86e2|Elysian Fields                            |2009-05-03 15:10:18|4e78efc4-e545-47af-9617-05ff816d86e2                  |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|ad0811ea-e213-451d-b22f-fa1a7f9e0226|Beanfield      |fb51d2c4-cc69-4128-92f5-77ec38d66859|Planetary Deadlock                        |2009-05-03 15:04:31|fb51d2c4-cc69-4128-92f5-77ec38d66859                  |m     |NULL|Japan  |Aug 13, 2006|\n",
      "|user_000001|309e2dfc-678e-4d09-a7a4-8eab9525b669|Dj Linus       |4277434f-e3c2-41ae-9ce3-23fd157f9347|Good Morning Love Coffee Is Ready         |2009-05-03 14:56:25|4277434f-e3c2-41ae-9ce3-23fd157f9347                  |m     |NULL|Japan  |Aug 13, 2006|\n",
      "+-----------+------------------------------------+---------------+------------------------------------+------------------------------------------+-------------------+------------------------------------------------------+------+----+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mprofileSchema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mSeq\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"user_id\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mfalse\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"gender\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"age\"\u001b[39m,\n",
       "    dataType = IntegerType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"country\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"signup\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  )\n",
       ")\n",
       "\u001b[36mprofileDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_id: string, gender: string ... 3 more fields]\n",
       "\u001b[36menrichedDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_id: string, artist_id: string ... 9 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val profileSchema = StructType(Seq(\n",
    "  StructField(\"user_id\", StringType, nullable = false),\n",
    "  StructField(\"gender\", StringType, nullable = true),\n",
    "  StructField(\"age\", IntegerType, nullable = true),\n",
    "  StructField(\"country\", StringType, nullable = true),\n",
    "  StructField(\"signup\", StringType, nullable = true)\n",
    "))\n",
    "\n",
    "val profileDf = spark.read\n",
    "  .option(\"sep\", \"\t\")\n",
    "  .option(\"header\", \"false\")\n",
    "  .schema(profileSchema)\n",
    "  .csv(PROFILE_PATH)\n",
    "\n",
    "val enrichedDf = dqDf.join(profileDf, Seq(\"user_id\"), \"left\")\n",
    "\n",
    "enrichedDf.show(SAMPLE_ROWS, truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb0df1-7125-49c8-a982-41714f898635",
   "metadata": {},
   "source": [
    "## 14) Wrap-up\n",
    "**Key takeaways:**\n",
    "- Data quality checks caught invalid timestamps, missing IDs, and empty fields.\n",
    "- Clear policies and documented assumptions make the pipeline reproducible.\n",
    "- Outputs (curated plays + metrics + profiles) are ready for downstream use in sessionization and top-track analysis.\n",
    "\n",
    "**End of Notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee78707b-7e87-4193-bcc1-4a7277aaa8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
